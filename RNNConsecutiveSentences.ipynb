{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[208,  79, 160],\n",
       "       [252,  26, 210]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "matrixA = np.array([[11,  3, 10, 3], [20, 1, 0, 1]])\n",
    "matrixB = np.array([[12,  1, 10], [7, 4, 0], [4, 5, 2], [5, 2, 10]])\n",
    "matrixA.dot(matrixB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbc37cafdb0>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import torchtext.vocab as vocab\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(nameFile, toSave):\n",
    "    pickle_out = open(nameFile+\".pickle\", \"wb\")\n",
    "    pickle.dump(toSave, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "def load(nameFile):\n",
    "    pickle_in = open(nameFile+\".pickle\", \"rb\")\n",
    "    return pickle.load(pickle_in)\n",
    "\n",
    "def main_iter_files():\n",
    "    print('Import data')\n",
    "    output_path = '/people/maurice/ownCloud/outputGentle/'\n",
    "    wordsTimeds = []\n",
    "    for file in sorted(glob.glob(output_path + '*')):\n",
    "        print(file)\n",
    "        if 'wordsTimed' in file and 'pickle' not in file:\n",
    "            wordsTimed = pd.read_csv(file)  # load(file)\n",
    "            print(wordsTimed.head())\n",
    "            wordsTimeds.append(wordsTimed)\n",
    "            # wordsTimedGby = wordsTimed.groupby('idSentence')\n",
    "            '''sentenceTimed = wordsTimedGby.apply(lambda x: x.count())\n",
    "            sentenceTimed[1] = sentenceTimed.astype(np.float)/len(g) \n",
    "            print sentenceTimed'''\n",
    "    return wordsTimeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import data\n",
      "/people/maurice/ownCloud/outputGentle/align.pickle\n",
      "/people/maurice/ownCloud/outputGentle/gentle_txt.pickle\n",
      "/people/maurice/ownCloud/outputGentle/gentle_txtS2E23.pickle\n",
      "/people/maurice/ownCloud/outputGentle/gentle_txtS2E5.pickle\n",
      "/people/maurice/ownCloud/outputGentle/gntle_end.pickle\n",
      "/people/maurice/ownCloud/outputGentle/gntle_endS2E23.pickle\n",
      "/people/maurice/ownCloud/outputGentle/gntle_endS2E5.pickle\n",
      "/people/maurice/ownCloud/outputGentle/gntle_start.pickle\n",
      "/people/maurice/ownCloud/outputGentle/gntle_startS2E23.pickle\n",
      "/people/maurice/ownCloud/outputGentle/gntle_startS2E5.pickle\n",
      "/people/maurice/ownCloud/outputGentle/old\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS1E1.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS1E10.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS1E11.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS1E12.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS1E13.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS1E14.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS1E15.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS1E16.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS1E17.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS1E2.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS1E3.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS1E4.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS1E5.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS1E6.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS1E7.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS1E8.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS1E9.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E1.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E10.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E11.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E12.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E13.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E14.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E15.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E16.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E17.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E18.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E19.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E2.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E20.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E21.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E22.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E3.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E4.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E5.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E6.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E7.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E8.csv\n",
      "/people/maurice/ownCloud/outputGentle/phonesTimesS2E9.csv\n",
      "/people/maurice/ownCloud/outputGentle/spacy_txt.pickle\n",
      "/people/maurice/ownCloud/outputGentle/spacy_txtS2E23.pickle\n",
      "/people/maurice/ownCloud/outputGentle/spacy_txtS2E5.pickle\n",
      "/people/maurice/ownCloud/outputGentle/speakers_words.pickle\n",
      "/people/maurice/ownCloud/outputGentle/who_speak.pickle\n",
      "/people/maurice/ownCloud/outputGentle/who_speakS2E23.pickle\n",
      "/people/maurice/ownCloud/outputGentle/who_speakS2E5.pickle\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS1E1.csv\n",
      "   Unnamed: 0    word  t_debut  t_fin  speaker  idSentence\n",
      "0           0      So     1.29   1.57  Sheldon           0\n",
      "1           1      if    -1.00  -1.00  Sheldon           0\n",
      "2           2       a     1.69   1.77  Sheldon           0\n",
      "3           3  photon     1.77   2.25  Sheldon           0\n",
      "4           4      is     2.26   2.37  Sheldon           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS1E10.csv\n",
      "   Unnamed: 0    word  t_debut  t_fin  speaker  idSentence\n",
      "0           0     See     2.58   2.83  Leonard           0\n",
      "1           1       ,     2.83   2.83  Leonard           0\n",
      "2           2     the     2.86   2.99  Leonard           0\n",
      "3           3  liquid     2.99   3.34  Leonard           0\n",
      "4           4   metal     3.34   3.62  Leonard           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS1E11.csv\n",
      "   Unnamed: 0       word  t_debut  t_fin  speaker  idSentence\n",
      "0           0  Checkmate    27.16  27.57  Sheldon           0\n",
      "1           1          .    27.57  27.57  Sheldon           0\n",
      "2           2          O    27.57  27.58  Leonard           1\n",
      "3           3          -    27.58  27.58  Leonard           1\n",
      "4           4          o    27.58  27.69  Leonard           1\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS1E12.csv\n",
      "   Unnamed: 0     word  t_debut  t_fin  speaker  idSentence\n",
      "0           0     Here     5.80  -1.00  Sheldon           0\n",
      "1           1       's    -1.00   6.07  Sheldon           0\n",
      "2           2      the     6.07   6.16  Sheldon           0\n",
      "3           3  problem     6.16   6.58  Sheldon           0\n",
      "4           4     with     6.58   6.69  Sheldon           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS1E13.csv\n",
      "   Unnamed: 0     word  t_debut  t_fin speaker  idSentence\n",
      "0           0      Ooh     2.03   2.19  Howard           0\n",
      "1           1        ,     2.19   2.19  Howard           0\n",
      "2           2     more     2.19   2.36  Howard           0\n",
      "3           3  details     2.36   2.68  Howard           0\n",
      "4           4    about     2.68   2.85  Howard           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS1E14.csv\n",
      "   Unnamed: 0      word  t_debut  t_fin  speaker  idSentence\n",
      "0           0      Well     1.86   2.00  Sheldon           0\n",
      "1           1         ,     2.00   2.00  Sheldon           0\n",
      "2           2      this     2.01   2.21  Sheldon           0\n",
      "3           3  sandwich     2.21   2.59  Sheldon           0\n",
      "4           4        is     2.60   2.68  Sheldon           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS1E15.csv\n",
      "   Unnamed: 0   word  t_debut  t_fin  speaker  idSentence\n",
      "0           0     On     1.52   1.54  Leonard           0\n",
      "1           1    the     1.54   1.66  Leonard           0\n",
      "2           2  other     1.66   1.87  Leonard           0\n",
      "3           3   hand     1.87   2.18  Leonard           0\n",
      "4           4      ,     2.18   2.18  Leonard           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS1E16.csv\n",
      "   Unnamed: 0  word  t_debut  t_fin speaker  idSentence\n",
      "0           0   Hey     6.46   6.57   Penny           0\n",
      "1           1     ,     6.57   6.57   Penny           0\n",
      "2           2  guys    -1.00  -1.00   Penny           0\n",
      "3           3     ,    -1.00  -1.00   Penny           0\n",
      "4           4  guys     7.65   7.73   Penny           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS1E17.csv\n",
      "   Unnamed: 0     word  t_debut  t_fin  speaker  idSentence\n",
      "0           0       Wo     2.26   2.54  Sheldon           0\n",
      "1           1       de     2.54   2.75  Sheldon           0\n",
      "2           2    zhing     2.76   3.06  Sheldon           0\n",
      "3           3      shi     3.27   3.54  Sheldon           0\n",
      "4           4  Sheldon     3.55   4.19  Sheldon           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS1E2.csv\n",
      "   Unnamed: 0   word  t_debut  t_fin  speaker  idSentence\n",
      "0           0  There     1.76   1.79  Leonard           0\n",
      "1           1    you     2.14   2.34  Leonard           0\n",
      "2           2     go     2.34   2.76  Leonard           0\n",
      "3           3      ,     2.76   2.76  Leonard           0\n",
      "4           4    Pad     2.80   3.06  Leonard           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS1E3.csv\n",
      "   Unnamed: 0     word  t_debut  t_fin speaker  idSentence\n",
      "0           0  Alright    10.50  11.12  Howard           0\n",
      "1           1        ,    11.12  11.12  Howard           0\n",
      "2           2     just    11.12  11.38  Howard           0\n",
      "3           3        a    11.38  11.46  Howard           0\n",
      "4           4      few    11.46  11.79  Howard           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS1E4.csv\n",
      "   Unnamed: 0      word  t_debut  t_fin  speaker  idSentence\n",
      "0           0         I     2.60  -1.00  Sheldon           0\n",
      "1           1       've    -1.00   2.69  Sheldon           0\n",
      "2           2      been     2.69   2.85  Sheldon           0\n",
      "3           3  thinking     2.85   3.19  Sheldon           0\n",
      "4           4     about     3.19   3.42  Sheldon           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS1E5.csv\n",
      "   Unnamed: 0     word  t_debut  t_fin  speaker  idSentence\n",
      "0           0  Alright     1.72   2.37  Sheldon           0\n",
      "1           1        ,     2.37   2.37  Sheldon           0\n",
      "2           2        I     2.54  -1.00  Sheldon           0\n",
      "3           3       'm    -1.00   2.83  Sheldon           0\n",
      "4           4   moving     2.83   3.18  Sheldon           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS1E6.csv\n",
      "   Unnamed: 0  word  t_debut  t_fin speaker  idSentence\n",
      "0           0  Okay     7.98   8.27     Raj           0\n",
      "1           1     ,     8.27   8.27     Raj           0\n",
      "2           2    if     8.27   8.40     Raj           0\n",
      "3           3    no     8.40   8.52     Raj           0\n",
      "4           4     -     8.52   8.52     Raj           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS1E7.csv\n",
      "   Unnamed: 0   word  t_debut  t_fin speaker  idSentence\n",
      "0           0  Watch     1.89   1.92  Howard           0\n",
      "1           1   this     1.92   2.18  Howard           0\n",
      "2           2      ,     2.18   2.18  Howard           0\n",
      "3           3     it     2.19  -1.00  Howard           0\n",
      "4           4     's    -1.00   2.34  Howard           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS1E8.csv\n",
      "   Unnamed: 0            word    t_debut      t_fin  speaker  idSentence\n",
      "0           0            Damn   9.559999   9.879999  Sheldon           0\n",
      "1           1             you   9.880000  10.060000  Sheldon           0\n",
      "2           2               ,  10.060000  10.060000  Sheldon           0\n",
      "3           3  walletnook.com  10.070000  11.390000  Sheldon           0\n",
      "4           4               .  11.390000  11.390000  Sheldon           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS1E9.csv\n",
      "   Unnamed: 0  word  t_debut  t_fin  speaker  idSentence\n",
      "0           0  Okay     1.66   2.38  Leonard           0\n",
      "1           1     ,     2.38   2.38  Leonard           0\n",
      "2           2   the     4.36   4.54  Leonard           0\n",
      "3           3  X10s     4.54   5.12  Leonard           0\n",
      "4           4   are     5.12   5.23  Leonard           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E1.csv\n",
      "   Unnamed: 0  word  t_debut  t_fin  speaker  idSentence\n",
      "0           0    So     1.26   1.43  Leonard           0\n",
      "1           1   you     1.43   1.54  Leonard           0\n",
      "2           2   see     1.54   1.69  Leonard           0\n",
      "3           3     ,     1.69   1.69  Leonard           0\n",
      "4           4  what     1.69   1.82  Leonard           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E10.csv\n",
      "   Unnamed: 0      word  t_debut  t_fin  speaker  idSentence\n",
      "0           0      Your     1.33   1.41  Sheldon           0\n",
      "1           1  argument     1.44   1.81  Sheldon           0\n",
      "2           2        is     1.83   2.03  Sheldon           0\n",
      "3           3   lacking     2.03   2.47  Sheldon           0\n",
      "4           4        in     2.47   2.59  Sheldon           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E11.csv\n",
      "   Unnamed: 0   word  t_debut  t_fin speaker  idSentence\n",
      "0           0    All     1.74   1.86  Howard           0\n",
      "1           1  right     1.86   2.32  Howard           0\n",
      "2           2      ,     2.32   2.32  Howard           0\n",
      "3           3   that     2.35  -1.00  Howard           0\n",
      "4           4     ’s    -1.00   2.65  Howard           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E12.csv\n",
      "   Unnamed: 0       word  t_debut  t_fin speaker  idSentence\n",
      "0           0        Mmm     3.10   3.59     Raj           0\n",
      "1           1          ,     3.59   3.59     Raj           0\n",
      "2           2  gentlemen     6.38   6.80     Raj           0\n",
      "3           3          ,     6.80   6.80     Raj           0\n",
      "4           4          I     6.92   7.07     Raj           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E13.csv\n",
      "   Unnamed: 0     word  t_debut  t_fin  speaker  idSentence\n",
      "0           0     Hmmm     3.49   3.80  Leonard           0\n",
      "1           1        .     3.80   3.80  Leonard           0\n",
      "2           2      The     5.30   5.42  Sheldon           1\n",
      "3           3  problem     5.42   5.69  Sheldon           1\n",
      "4           4  appears     5.73   5.93  Sheldon           1\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E14.csv\n",
      "   Unnamed: 0    word    t_debut      t_fin speaker  idSentence\n",
      "0           0  Fellas  16.449999  16.569999   Penny           0\n",
      "1           1       ,  16.569999  16.569999   Penny           0\n",
      "2           2  please  28.330000  29.160000   Penny           0\n",
      "3           3       .  29.160000  29.160000   Penny           0\n",
      "4           4   Penny  29.320000  29.720000  Howard           1\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E15.csv\n",
      "   Unnamed: 0   word  t_debut  t_fin speaker  idSentence\n",
      "0           0   That     1.57   1.62  Howard           0\n",
      "1           1    was     3.12   3.30  Howard           0\n",
      "2           2  close     3.30   3.61  Howard           0\n",
      "3           3      .     3.61   3.61  Howard           0\n",
      "4           4    God     6.39   6.70     Raj           1\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E16.csv\n",
      "   Unnamed: 0  word  t_debut  t_fin speaker  idSentence\n",
      "0           0  Okay     1.48   1.69  Howard           0\n",
      "1           1     ,     1.69   1.69  Howard           0\n",
      "2           2   Raj     2.81   3.16  Howard           0\n",
      "3           3     ,     3.16   3.16  Howard           0\n",
      "4           4  hand     3.76   3.96  Howard           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E17.csv\n",
      "   Unnamed: 0    word  t_debut  t_fin speaker  idSentence\n",
      "0           0  Knight     1.30   1.88     Raj           0\n",
      "1           1      to     1.95   2.12     Raj           0\n",
      "2           2   queen     2.12  -1.00     Raj           0\n",
      "3           3      ’s    -1.00   2.61     Raj           0\n",
      "4           4  bishop     2.62   3.08     Raj           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E18.csv\n",
      "   Unnamed: 0  word  t_debut  t_fin speaker  idSentence\n",
      "0           0  Okay     1.25   1.49   Penny           0\n",
      "1           1     ,     1.49   1.49   Penny           0\n",
      "2           2    it     1.86  -1.00   Penny           0\n",
      "3           3    ’s    -1.00   2.13   Penny           0\n",
      "4           4  done     2.13   2.71   Penny           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E19.csv\n",
      "   Unnamed: 0 word  t_debut  t_fin  speaker  idSentence\n",
      "0           0   Hu     3.73   3.80  Sheldon           0\n",
      "1           1    -     3.80   3.80  Sheldon           0\n",
      "2           2    u    -1.00  -1.00  Sheldon           0\n",
      "3           3    -    -1.00  -1.00  Sheldon           0\n",
      "4           4    u    11.17  11.25  Sheldon           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E2.csv\n",
      "   Unnamed: 0         word  t_debut  t_fin  speaker  idSentence\n",
      "0           0        Worst     2.86   3.43  Sheldon           0\n",
      "1           1  Renaissance     3.43   4.25  Sheldon           0\n",
      "2           2         Fair     4.26   4.88  Sheldon           0\n",
      "3           3         ever     4.99   5.12  Sheldon           0\n",
      "4           4            .     5.12   5.12  Sheldon           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E20.csv\n",
      "   Unnamed: 0  word  t_debut  t_fin speaker  idSentence\n",
      "0           0  Okay     1.38   1.67  Howard           0\n",
      "1           1     ,     1.67   1.67  Howard           0\n",
      "2           2   are     2.27   2.43  Howard           0\n",
      "3           3   you     2.43   3.03  Howard           0\n",
      "4           4  from     3.05   3.23  Howard           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E21.csv\n",
      "   Unnamed: 0   word  t_debut  t_fin  speaker  idSentence\n",
      "0           0  Smell     7.44   7.72  Sheldon           0\n",
      "1           1   that     7.72   8.04  Sheldon           0\n",
      "2           2      ?     8.04   8.04  Sheldon           0\n",
      "3           3   That     8.86  -1.00  Sheldon           0\n",
      "4           4     ’s    -1.00   9.00  Sheldon           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E22.csv\n",
      "   Unnamed: 0  word  t_debut  t_fin  speaker  idSentence\n",
      "0           0    Oh     3.98   4.32  Sheldon           0\n",
      "1           1     ,     4.32   4.32  Sheldon           0\n",
      "2           2   boy     4.32   4.80  Sheldon           0\n",
      "3           3     .     4.80   4.80  Sheldon           0\n",
      "4           4  What     7.26   7.82  Leonard           1\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E3.csv\n",
      "   Unnamed: 0      word  t_debut  t_fin  speaker  idSentence\n",
      "0           0    Fellow     1.77   2.15  Sheldon           0\n",
      "1           1  warriors     2.25   2.82  Sheldon           0\n",
      "2           2         ,     2.82   2.82  Sheldon           0\n",
      "3           3      this     3.29   3.44  Sheldon           0\n",
      "4           4        is     3.44   3.51  Sheldon           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E4.csv\n",
      "   Unnamed: 0 word  t_debut  t_fin  speaker  idSentence\n",
      "0           0  Let     1.25  -1.00  Leonard           0\n",
      "1           1   ’s    -1.00   1.47  Leonard           0\n",
      "2           2  see     1.47   1.67  Leonard           0\n",
      "3           3    ,     1.67   1.67  Leonard           0\n",
      "4           4  Raj     1.67   2.13  Leonard           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E5.csv\n",
      "   Unnamed: 0     word  t_debut  t_fin  speaker  idSentence\n",
      "0           0     Good    10.86  11.08  Sheldon           0\n",
      "1           1  morning    11.08  11.36  Sheldon           0\n",
      "2           2        ,    11.36  11.36  Sheldon           0\n",
      "3           3  Leonard    11.53  11.75  Sheldon           0\n",
      "4           4        .    11.75  11.75  Sheldon           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E6.csv\n",
      "   Unnamed: 0 word  t_debut  t_fin  speaker  idSentence\n",
      "0           0   So     1.32   1.61  Leonard           0\n",
      "1           1    ,     1.61   1.61  Leonard           0\n",
      "2           2   if     1.61   1.75  Leonard           0\n",
      "3           3  any     1.75   1.91  Leonard           0\n",
      "4           4   of     1.91   2.01  Leonard           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E7.csv\n",
      "   Unnamed: 0      word  t_debut  t_fin  speaker  idSentence\n",
      "0           0      Time     -1.0   -1.0  Sheldon           0\n",
      "1           1         .     -1.0   -1.0  Sheldon           0\n",
      "2           2   Alright     -1.0   -1.0  Sheldon           0\n",
      "3           3  Klingons     -1.0   -1.0  Sheldon           0\n",
      "4           4         ,     -1.0   -1.0  Sheldon           0\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E8.csv\n",
      "   Unnamed: 0   word  t_debut  t_fin  speaker  idSentence\n",
      "0           0  Penny     1.68   2.07  Sheldon           0\n",
      "1           1      ,     2.07   2.07  Sheldon           0\n",
      "2           2  hello     2.47   3.00  Sheldon           0\n",
      "3           3      .     3.00   3.00  Sheldon           0\n",
      "4           4    Hey     3.69   3.78    Penny           1\n",
      "/people/maurice/ownCloud/outputGentle/wordsTimedS2E9.csv\n",
      "   Unnamed: 0      word  t_debut  t_fin speaker  idSentence\n",
      "0           0         I     4.57   4.69   Steph           0\n",
      "1           1        do     4.69  -1.00   Steph           0\n",
      "2           2       n’t    -1.00   4.87   Steph           0\n",
      "3           3       see     4.87   5.05   Steph           0\n",
      "4           4  anything    -1.00  -1.00   Steph           0\n"
     ]
    }
   ],
   "source": [
    "wts = main_iter_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class sentenceTimed(object):\n",
    "    def __init__(self, wt):\n",
    "        self.reset(0)\n",
    "        \n",
    "    def reset(self, i):\n",
    "        self.speaker = wt.iloc[i].speaker\n",
    "        self.sentence_courante = ''\n",
    "        if i > 0:\n",
    "            self.sentence_courante += wt.iloc[i].word\n",
    "        \n",
    "    def modif_per_word(self, wt, i):\n",
    "        self.sentence_courante += ' ' + wt.iloc[i].word\n",
    "\n",
    "    def modif_per_sentence(self, wt, df, i):\n",
    "        self.add_sentence_informations_to_dataframe(df)\n",
    "        self.reset(i)\n",
    "        \n",
    "    def add_sentence_informations_to_dataframe(self, df):\n",
    "        df.loc[len(df)] = [self.speaker, self.sentence_courante]\n",
    "        # print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "# Lent\n",
    "punctuation_end_sentence = ['!', '.', '?']\n",
    "sentencesTimeds = []\n",
    "print(len(wts))\n",
    "for i, wt in enumerate(wts):\n",
    "    print(i)\n",
    "    sentencesTimed = pd.DataFrame(columns=['speaker', 'sentence_courante'])    \n",
    "\n",
    "    st = sentenceTimed(wt)\n",
    "\n",
    "    for word in range(len(wt)):\n",
    "        if word == 0:\n",
    "            st.modif_per_word(wt, word)\n",
    "        elif wt.iloc[word].word[0].isupper() and wt.iloc[word - 1].word in punctuation_end_sentence:\n",
    "            st.modif_per_sentence(wt, sentencesTimed, word)\n",
    "        else:\n",
    "            st.modif_per_word(wt, word)\n",
    "\n",
    "    st.add_sentence_informations_to_dataframe(sentencesTimed)\n",
    "    sentencesTimeds.append(sentencesTimed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>sentence_courante</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>So if a photon is directed through a plane wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>If it 's unobserved it will , however , if it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Agreed , what 's your point ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>There 's no point , I just think it 's a good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Excuse me ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Receptionist</td>\n",
       "      <td>Hang on .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>One across is Aegean , eight down is Nabakov ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>See , Papa Doc 's capital idea , that 's Port ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Haiti .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Receptionist</td>\n",
       "      <td>Can I help you ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Yes .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Um , is this the High IQ sperm bank ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Receptionist</td>\n",
       "      <td>If you have to ask , maybe you should n't be h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>I think this is the place .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Receptionist</td>\n",
       "      <td>Fill these out .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Thank - you .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>We 'll be right back .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Receptionist</td>\n",
       "      <td>Oh , take your time .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Receptionist</td>\n",
       "      <td>I 'll just finish my crossword puzzle .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Receptionist</td>\n",
       "      <td>Oh wait .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Leonard , I do n't think I can do this .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>What , are you kidding ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>You 're a semi - pro .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>No .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>We are committing genetic fraud .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>There 's no guarantee that our sperm is going ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>I have a sister with the same basic DNA mix wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Sheldon , this was your idea .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>A little extra money to get fractional T1 band...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>I know , and I do yearn for faster downloads ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>We 're home .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>Penny</td>\n",
       "      <td>Oh , my God , what happened ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Well , your ex - boyfriend sends his regards a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>Penny</td>\n",
       "      <td>I 'm so sorry , I really thought if you guys w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>No , it was a valid hypothesis .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>That was a valid hypothesis ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>What is happening to you ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>Penny</td>\n",
       "      <td>Really , thank you so much for going and tryin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>Penny</td>\n",
       "      <td>Why do n't you put some clothes on , I 'll get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Really ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Great .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Thank you .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>You 're not done with her , are you ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Our babies will be smart and beautiful .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Not to mention imaginary .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Is Thai food okay with you Penny ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>Penny</td>\n",
       "      <td>Sure .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>We ca n't have Thai food , we had Indian for l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>Penny</td>\n",
       "      <td>So ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>They 're both curry based cuisines .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>Penny</td>\n",
       "      <td>So ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>They would be gastronomically redundant .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>I can see we 're going to have to spell out ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>Penny</td>\n",
       "      <td>Any ideas Raj ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>Howard</td>\n",
       "      <td>Turn left on Lake Street and head up to Colora...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>Howard</td>\n",
       "      <td>I know a wonderful little sushi bar that has k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>Penny</td>\n",
       "      <td>That sounds like fun .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>Howard</td>\n",
       "      <td>Baby , baby do n't get hooked on me .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>Howard</td>\n",
       "      <td>Uh , baby , baby do n't get hooked on me .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>I do n't know what your odds are in the world ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>409 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          speaker                                  sentence_courante\n",
       "0         Sheldon   So if a photon is directed through a plane wi...\n",
       "1         Sheldon  If it 's unobserved it will , however , if it ...\n",
       "2         Leonard                      Agreed , what 's your point ?\n",
       "3         Sheldon  There 's no point , I just think it 's a good ...\n",
       "4         Leonard                                        Excuse me ?\n",
       "5    Receptionist                                          Hang on .\n",
       "6         Leonard  One across is Aegean , eight down is Nabakov ,...\n",
       "7         Leonard  See , Papa Doc 's capital idea , that 's Port ...\n",
       "8         Leonard                                            Haiti .\n",
       "9    Receptionist                                   Can I help you ?\n",
       "10        Leonard                                              Yes .\n",
       "11        Leonard              Um , is this the High IQ sperm bank ?\n",
       "12   Receptionist  If you have to ask , maybe you should n't be h...\n",
       "13        Sheldon                        I think this is the place .\n",
       "14   Receptionist                                   Fill these out .\n",
       "15        Leonard                                      Thank - you .\n",
       "16        Leonard                             We 'll be right back .\n",
       "17   Receptionist                              Oh , take your time .\n",
       "18   Receptionist            I 'll just finish my crossword puzzle .\n",
       "19   Receptionist                                          Oh wait .\n",
       "20        Sheldon           Leonard , I do n't think I can do this .\n",
       "21        Leonard                           What , are you kidding ?\n",
       "22        Leonard                             You 're a semi - pro .\n",
       "23        Sheldon                                               No .\n",
       "24        Sheldon                  We are committing genetic fraud .\n",
       "25        Sheldon  There 's no guarantee that our sperm is going ...\n",
       "26        Sheldon  I have a sister with the same basic DNA mix wh...\n",
       "27        Leonard                     Sheldon , this was your idea .\n",
       "28        Leonard  A little extra money to get fractional T1 band...\n",
       "29        Sheldon  I know , and I do yearn for faster downloads ,...\n",
       "..            ...                                                ...\n",
       "379       Leonard                                      We 're home .\n",
       "380         Penny                      Oh , my God , what happened ?\n",
       "381       Leonard  Well , your ex - boyfriend sends his regards a...\n",
       "382         Penny  I 'm so sorry , I really thought if you guys w...\n",
       "383       Leonard                   No , it was a valid hypothesis .\n",
       "384       Sheldon                      That was a valid hypothesis ?\n",
       "385       Sheldon                         What is happening to you ?\n",
       "386         Penny  Really , thank you so much for going and tryin...\n",
       "387         Penny  Why do n't you put some clothes on , I 'll get...\n",
       "388       Leonard                                           Really ?\n",
       "389       Leonard                                            Great .\n",
       "390       Sheldon                                        Thank you .\n",
       "391       Sheldon              You 're not done with her , are you ?\n",
       "392       Leonard           Our babies will be smart and beautiful .\n",
       "393       Sheldon                         Not to mention imaginary .\n",
       "394       Leonard                 Is Thai food okay with you Penny ?\n",
       "395         Penny                                             Sure .\n",
       "396       Sheldon  We ca n't have Thai food , we had Indian for l...\n",
       "397         Penny                                               So ?\n",
       "398       Sheldon               They 're both curry based cuisines .\n",
       "399         Penny                                               So ?\n",
       "400       Sheldon          They would be gastronomically redundant .\n",
       "401       Sheldon  I can see we 're going to have to spell out ev...\n",
       "402         Penny                                    Any ideas Raj ?\n",
       "403        Howard  Turn left on Lake Street and head up to Colora...\n",
       "404        Howard  I know a wonderful little sushi bar that has k...\n",
       "405         Penny                             That sounds like fun .\n",
       "406        Howard              Baby , baby do n't get hooked on me .\n",
       "407        Howard         Uh , baby , baby do n't get hooked on me .\n",
       "408       Sheldon  I do n't know what your odds are in the world ...\n",
       "\n",
       "[409 rows x 2 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencesTimeds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speaker                                                        Sheldon\n",
       "sentence_courante    If it 's unobserved it will , however , if it ...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencesTimeds[0].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sheldon'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencesTimeds[0].iloc[1].speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If it 's unobserved it will , however , if it 's observed after it 's left the plane but before it hits its target , it will not have gone through both slits .\""
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencesTimeds[0].iloc[1].sentence_courante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.concat(sentencesTimeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>sentence_courante</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>So if a photon is directed through a plane wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>If it 's unobserved it will , however , if it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Agreed , what 's your point ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>There 's no point , I just think it 's a good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Excuse me ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Receptionist</td>\n",
       "      <td>Hang on .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>One across is Aegean , eight down is Nabakov ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>See , Papa Doc 's capital idea , that 's Port ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Haiti .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Receptionist</td>\n",
       "      <td>Can I help you ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Yes .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Um , is this the High IQ sperm bank ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Receptionist</td>\n",
       "      <td>If you have to ask , maybe you should n't be h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>I think this is the place .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Receptionist</td>\n",
       "      <td>Fill these out .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Thank - you .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>We 'll be right back .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Receptionist</td>\n",
       "      <td>Oh , take your time .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Receptionist</td>\n",
       "      <td>I 'll just finish my crossword puzzle .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Receptionist</td>\n",
       "      <td>Oh wait .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Leonard , I do n't think I can do this .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>What , are you kidding ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>You 're a semi - pro .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>No .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>We are committing genetic fraud .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>There 's no guarantee that our sperm is going ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>I have a sister with the same basic DNA mix wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Sheldon , this was your idea .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>A little extra money to get fractional T1 band...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sheldon</td>\n",
       "      <td>I know , and I do yearn for faster downloads ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>Raj</td>\n",
       "      <td>Hey .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Howard</td>\n",
       "      <td>Hey .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Howard</td>\n",
       "      <td>Nice sweater .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Yeah , Stephanie got it for me .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>It ’s kind of fun .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>Raj</td>\n",
       "      <td>It ’s got a big bird on it , dude .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Yeah , yeah , that ’s the fun part .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>We ’re also getting new curtains for my bedroo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>Raj</td>\n",
       "      <td>You ’re lucky .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Raj</td>\n",
       "      <td>With me , it ’s usually the other way around .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>Howard</td>\n",
       "      <td>You know , if you ca n’t talk to her , why do ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Is n’t that kind of cowardly ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>Howard</td>\n",
       "      <td>Oh , yeah .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Howard</td>\n",
       "      <td>It ’s beyond contemptible .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>Raj</td>\n",
       "      <td>It ’s true , but on the other hand you are wea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Sold . “ I think it would be better for our re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>It ’s done .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>Howard</td>\n",
       "      <td>Good for you .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Yeah , good for me .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>I ’ll never have sex again .   I was wrong .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>See ya .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>Penny</td>\n",
       "      <td>Sheldon ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Computer voice</td>\n",
       "      <td>I have an inflamed larynx .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Penny</td>\n",
       "      <td>Okay ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>Computer voice</td>\n",
       "      <td>We ’re out of herbal tea .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Computer voice</td>\n",
       "      <td>Do you have any ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>Penny</td>\n",
       "      <td>Okay , let me check .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>Computer voice</td>\n",
       "      <td>Some hiney would be nice , too .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>Penny</td>\n",
       "      <td>Hiney ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>Computer voice</td>\n",
       "      <td>Honey .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12702 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            speaker                                  sentence_courante\n",
       "0           Sheldon   So if a photon is directed through a plane wi...\n",
       "1           Sheldon  If it 's unobserved it will , however , if it ...\n",
       "2           Leonard                      Agreed , what 's your point ?\n",
       "3           Sheldon  There 's no point , I just think it 's a good ...\n",
       "4           Leonard                                        Excuse me ?\n",
       "5      Receptionist                                          Hang on .\n",
       "6           Leonard  One across is Aegean , eight down is Nabakov ,...\n",
       "7           Leonard  See , Papa Doc 's capital idea , that 's Port ...\n",
       "8           Leonard                                            Haiti .\n",
       "9      Receptionist                                   Can I help you ?\n",
       "10          Leonard                                              Yes .\n",
       "11          Leonard              Um , is this the High IQ sperm bank ?\n",
       "12     Receptionist  If you have to ask , maybe you should n't be h...\n",
       "13          Sheldon                        I think this is the place .\n",
       "14     Receptionist                                   Fill these out .\n",
       "15          Leonard                                      Thank - you .\n",
       "16          Leonard                             We 'll be right back .\n",
       "17     Receptionist                              Oh , take your time .\n",
       "18     Receptionist            I 'll just finish my crossword puzzle .\n",
       "19     Receptionist                                          Oh wait .\n",
       "20          Sheldon           Leonard , I do n't think I can do this .\n",
       "21          Leonard                           What , are you kidding ?\n",
       "22          Leonard                             You 're a semi - pro .\n",
       "23          Sheldon                                               No .\n",
       "24          Sheldon                  We are committing genetic fraud .\n",
       "25          Sheldon  There 's no guarantee that our sperm is going ...\n",
       "26          Sheldon  I have a sister with the same basic DNA mix wh...\n",
       "27          Leonard                     Sheldon , this was your idea .\n",
       "28          Leonard  A little extra money to get fractional T1 band...\n",
       "29          Sheldon  I know , and I do yearn for faster downloads ,...\n",
       "..              ...                                                ...\n",
       "295             Raj                                              Hey .\n",
       "296          Howard                                              Hey .\n",
       "297          Howard                                     Nice sweater .\n",
       "298         Leonard                   Yeah , Stephanie got it for me .\n",
       "299         Leonard                                It ’s kind of fun .\n",
       "300             Raj                It ’s got a big bird on it , dude .\n",
       "301         Leonard               Yeah , yeah , that ’s the fun part .\n",
       "302         Leonard  We ’re also getting new curtains for my bedroo...\n",
       "303             Raj                                    You ’re lucky .\n",
       "304             Raj     With me , it ’s usually the other way around .\n",
       "305          Howard  You know , if you ca n’t talk to her , why do ...\n",
       "306         Leonard                     Is n’t that kind of cowardly ?\n",
       "307          Howard                                        Oh , yeah .\n",
       "308          Howard                        It ’s beyond contemptible .\n",
       "309             Raj  It ’s true , but on the other hand you are wea...\n",
       "310         Leonard  Sold . “ I think it would be better for our re...\n",
       "311         Leonard                                       It ’s done .\n",
       "312          Howard                                     Good for you .\n",
       "313         Leonard                               Yeah , good for me .\n",
       "314         Leonard       I ’ll never have sex again .   I was wrong .\n",
       "315         Leonard                                           See ya .\n",
       "316           Penny                                          Sheldon ?\n",
       "317  Computer voice                        I have an inflamed larynx .\n",
       "318           Penny                                             Okay ?\n",
       "319  Computer voice                         We ’re out of herbal tea .\n",
       "320  Computer voice                                  Do you have any ?\n",
       "321           Penny                              Okay , let me check .\n",
       "322  Computer voice                   Some hiney would be nice , too .\n",
       "323           Penny                                            Hiney ?\n",
       "324  Computer voice                                            Honey .\n",
       "\n",
       "[12702 rows x 2 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sheldon', 'Sheldon', 'Leonard', ..., 'Computer voice', 'Penny',\n",
       "       'Computer voice'], dtype=object)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.speaker.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits .',\n",
       "       \"If it 's unobserved it will , however , if it 's observed after it 's left the plane but before it hits its target , it will not have gone through both slits .\",\n",
       "       \"Agreed , what 's your point ?\", ...,\n",
       "       'Some hiney would be nice , too .', 'Hiney ?', 'Honey .'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentence_courante.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [s.lower().split() for s in data.sentence_courante.values]\n",
    "Y = [s.lower() for s in data.speaker.values]\n",
    "newY = np.zeros(len(Y)-1)\n",
    "for i in range(1, len(Y)):\n",
    "    if Y[i] == Y[i-1]:\n",
    "        newY[i-1] = 0\n",
    "    else:\n",
    "        newY[i-1] = 1\n",
    "Y = Variable(torch.from_numpy(newY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [s.lower().split() for s in data.sentence_courante.values]\n",
    "Y = [s.lower() for s in data.speaker.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [s.lower().split() for s in data.sentence_courante.values]\n",
    "Y = [s.lower() for s in data.speaker.values]\n",
    "newY = []\n",
    "for i in range(1, len(Y)):\n",
    "    if Y[i] == Y[i-1]:\n",
    "        newY.append([0])\n",
    "    else:\n",
    "        newY.append([1])\n",
    "Y = Variable(torch.FloatTensor(newY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['so', 'if', 'a', 'photon', 'is', 'directed', 'through', 'a', 'plane', 'with', 'two', 'slits', 'in', 'it', 'and', 'either', 'slit', 'is', 'observed', 'it', 'will', 'not', 'go', 'through', 'both', 'slits', '.'], ['if', 'it', \"'s\", 'unobserved', 'it', 'will', ',', 'however', ',', 'if', 'it', \"'s\", 'observed', 'after', 'it', \"'s\", 'left', 'the', 'plane', 'but', 'before', 'it', 'hits', 'its', 'target', ',', 'it', 'will', 'not', 'have', 'gone', 'through', 'both', 'slits', '.']]\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        ...,\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "torch.Size([12701, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X[0:2])\n",
    "print(Y)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12702 12701\n",
      "10161 10160 1270 1269 1271 1269\n"
     ]
    }
   ],
   "source": [
    "#X_train, X_dt, Y_train, Y_dt = train_test_split(X, Y, test_size=0.2, shuffle=False)\n",
    "print(len(X), len(Y))\n",
    "def create_Y(Y):\n",
    "    newY = []\n",
    "    for i in range(1, len(Y)):\n",
    "        if Y[i] == Y[i-1]:\n",
    "            newY.append([0])\n",
    "        else:\n",
    "            newY.append([1])\n",
    "    Y_new = Variable(torch.FloatTensor(newY))\n",
    "    return Y_new\n",
    "\n",
    "threshold_train_dev = int(len(X)*0.8)\n",
    "threshold_dev_test = threshold_train_dev + int(len(X)*0.1)\n",
    "X_train = X[:threshold_train_dev]\n",
    "Y_train = create_Y(Y[:threshold_train_dev])\n",
    "X_dev = X[threshold_train_dev:threshold_dev_test]\n",
    "Y_dev = create_Y(Y[threshold_train_dev:threshold_dev_test])\n",
    "X_test = X[threshold_dev_test:]\n",
    "Y_test = create_Y(Y[threshold_dev_test:])\n",
    "print(len(X_train), len(Y_train), len(X_dev), len(Y_dev), len(X_test), len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#embed = nn.Embedding(num_embeddings, embedding_dim)\n",
    "# pretrained_weight is a numpy matrix of shape (num_embeddings, embedding_dim)\n",
    "#embed.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "\n",
    "#we = vocab.GloVe(name='6B', dim=100)\n",
    "we = vocab.FastText(language='en')\n",
    "'''\n",
    "pretrained_aliases = {\n",
    "    \"charngram.100d\": partial(CharNGram),\n",
    "    \"fasttext.en.300d\": partial(FastText, language=\"en\"),\n",
    "    \"fasttext.simple.300d\": partial(FastText, language=\"simple\"),\n",
    "    \"glove.42B.300d\": partial(GloVe, name=\"42B\", dim=\"300\"),\n",
    "    \"glove.840B.300d\": partial(GloVe, name=\"840B\", dim=\"300\"),\n",
    "    \"glove.twitter.27B.25d\": partial(GloVe, name=\"twitter.27B\", dim=\"25\"),\n",
    "    \"glove.twitter.27B.50d\": partial(GloVe, name=\"twitter.27B\", dim=\"50\"),\n",
    "    \"glove.twitter.27B.100d\": partial(GloVe, name=\"twitter.27B\", dim=\"100\"),\n",
    "    \"glove.twitter.27B.200d\": partial(GloVe, name=\"twitter.27B\", dim=\"200\"),\n",
    "    \"glove.6B.50d\": partial(GloVe, name=\"6B\", dim=\"50\"),\n",
    "    \"glove.6B.100d\": partial(GloVe, name=\"6B\", dim=\"100\"),\n",
    "    \"glove.6B.200d\": partial(GloVe, name=\"6B\", dim=\"200\"),\n",
    "    \"glove.6B.300d\": partial(GloVe, name=\"6B\", dim=\"300\")\n",
    "}\n",
    "'''\n",
    "\n",
    "def get_word_vector(word):\n",
    "    return we.vectors[we.stoi[word]]\n",
    "\n",
    "def closest(vec, n=2):#10):\n",
    "    \"\"\"\n",
    "    Find the closest words for a given vector\n",
    "    \"\"\"\n",
    "    all_dists = [(w, torch.dist(vec, get_word_vector(w))) for w in we.itos]\n",
    "    return sorted(all_dists, key=lambda t: t[1])[:n]\n",
    "\n",
    "def print_tuples(tuples):\n",
    "    for tuple in tuples:\n",
    "        print('(%.4f) %s' % (tuple[1], tuple[0]))\n",
    "\n",
    "# In the form w1 : w2 :: w3 : ?\n",
    "def analogy(w1, w2, w3, n=5, filter_given=True):\n",
    "    print('\\n[%s : %s :: %s : ?]' % (w1, w2, w3))\n",
    "   \n",
    "    # w2 - w1 + w3 = w4\n",
    "    closest_words = closest(get_word_vector(w2) - get_word_vector(w1) + get_word_vector(w3))\n",
    "    \n",
    "    # Optionally filter out given words\n",
    "    if filter_given:\n",
    "        closest_words = [t for t in closest_words if t[0] not in [w1, w2, w3]]\n",
    "\n",
    "    print_tuples(closest_words[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lent\n",
    "print(we.dim,'\\n')\n",
    "print_tuples(closest(get_word_vector('google')))\n",
    "analogy('king', 'man', 'queen')\n",
    "print(type(we.stoi),'\\n')\n",
    "a = 0\n",
    "for k, v in we.stoi.items():\n",
    "    if a < 10:\n",
    "        print(k,v)\n",
    "        a += 1\n",
    "    else:\n",
    "        break\n",
    "print('iazkcnzejjqsdchj' in we.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#print(get_word_vector('iazkcnzejjqsdchj'))\n",
    "print('iazkcnzejjqsdchj' in we.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392 {'r.', 'allergenics', 'c’mon', '’re', 'swrap', 'mimeaux', '2.6', 'apprec', 'remembe', 'pkshhhh', '1500s', '1979', 'mcflono', '13-year', 'l.a.', 'heteronic', 'mr.', 'phd.', '15,000', 'farminfarmian', \"n't\", '5027.3', '120', 'neeeeeoooooowwwww', 'googenfeil', 'b.', 'becauth', 'whatchacall', 'a.m.', 'nekhmakh', '3400', 'slurpie', '45', '40-mile', 'unravelable', 'cherrrkh', 'thwow', 'weducing', 'tootin’', 'benedryl', '211', '1487', '7', '18th', '30', '1908', 'quantii', 'brissket', 'crazy?what', 'costcos', '1956', 'bearclaws', 'eggoes', 'night.</i', '27', 'acetaline', 'yarmulkah', '15', '1175', 'finkleday', '99', 'chattee', '1150', '9', 'batcrap', '4,390', '64', 'indoorsy', 'misunder', '’s', 'underreacting', 'c.', 'tomowow', 'punchali', 'n=4', 'a.k.a', \"'cos\", 'carragenin', '4-a.', '12', 'cocktain', '1', 'parents’', '28', 'v.', '25th', 'pwoton', \"'d\", 'x10', '4.1855', 'lookin’', '22-year', 'a.', 'woboto', '4.8', 'stweet', '’ve', '17', '12th', '35', 'd’être', 'hoffstadter', '640', '16', 'watest', 'yousa', '1778', 'n’t', '99-seat', '98', 'tordage', 'at?you’ve', 'm.', 'elzebub', 'sawami', 'alwight', 'sluttiness', '383', 'quonko', 'knotsbury', 'pumiced', '1980', '12.50', 'unbag', '11', '5,000', 'x.', 'pchew', 'appreeee', \"dit'nt\", 'sheldonectomy', 'awanged', 'www.socalphysicsgroup.org/activities/other', 'to’ing', 'polmerry', '20', 'vh1', '.15', 'anobo', 'e-harmony.com', \"'ll\", '800', '1930', 'o’clock', 'expewiment', '1974', 'intewest', 'webchatting', '7-eleven', 'can’t', '’bout', 'juvenilizing', \"o'clock\", '200', '1935', 'y.', '62', 'fo’', '3.95', '68', 'fertiliziation', '1.8', 'pwan', 'pwoblem', '32', 'fro’ing', '760', '802.11n', '20-channel', 'boppidy', '1876', 'licquorish', 'charmalarmalon', '40', 'menushya', 'woxanne', 'e.v.a.', 'qu’est', 'guawantee', '4.9', 'cwimb', 'darmspülung', '..', 'supewiowity', 'wolowizard', 'strippergram', '320', 'fwoss', 'weapeons', 'friggin’', '1948', 'f.l.aw.aw', '’em', 'koothrapali', '12.5', \"'cause\", '’ll', 'schmort', 'i.', '0700', '50', 'l.a', '500', 'meatlover', '8', 'wesults', '46.9', 't1', 'wobots', '’d', '140', 'weady', '187', 'sheldor', '104', '18', 'kapla’', 'bilurrrbe', '’cause', '25', '101', 'cweepy', '2008', 'humormometer', \"'m\", 'uurgh', '51', 'e.', 'schmilepton', \"'ve\", 'i.e', 'aaaaaaaaaaaaaa', '2', 'e.m.', '55', '60', '’m', '5,812', 'wealthybigpenis', 's’u’up', \"'re\", '06', '5.0', 'cherusalem', 'wubber', 'widicule', '5.1', '67', '4,000', '110', '2099', 'dewusions', 'contwol', 'mcfloonyloo', '5a', 'greaseboards', 'mytosis', '52', 'bjow', 'c3po', '2.0', 'aaaaaaaaaaaaaaaaaa', '6', '1,000', '2006', 'isospan', '2311', \"'em\", 'ekairidium', 'd.', 'ps3', '”', 'qochbe’', '4,400', '170', 'q’wass', 'snoodled', 'donkulous', 's.c.aw', 'flankenzie', '1992', 'dr.', 'mrs.', 'sssshhhh', '10th', 'pokh', '5.19', 'slurpies', 'pwasma', '26', 'fuzzyboots', 'bogarding', 'i.e.', 'bippidy', 'wocks', 'eeuh', 'neeeeoooowwwww', 'boys’', '21st', '29', 'friendmaking', 'l.h.', 'x10s', 'fwiend', 'walletnook.com', '3rd', '24', '5', 'l.', 'loompahs', 'kwipke', 'pthththth', 'o’brien', 'hewo', 'chorrr', '2328', 'afwaid', '3', 'anythingforagreencard.com', 'railways’', 'endorphic', 'pruny', 'talkin’', 'outearn', '380', 'ladies’', '156', 'freakin’', 'entewing', 'thwee', \"ma'am\", '350', 'cornhusking', 'möchtest', '1999', 'habilus', 'wobin', '128', 'gablehouser', 'bawy', 'kwippler', 'sezchuan', '212', 'spewers', \"y'\", '14', 'cavawier', '360', 'bubbula', '2a', 'under-30', 'potl', 'me.7', '22', 'impwessive', 'senoran', '130', '165', '...', '90', 'cheeseless', \"'s\", 'cwazy', '“', 'rhineheitsgebot', ';', 'elpful', 'mowon', '24/7', 'you’re', '80', '840', '36', '75', 'pre-1980', '4', 'mizvah', 'n=8', '204', 'oooaw', '3,000', 'quizznos', 'adoseconds', '23', 'neeeeeooooowwwwww', 'medpack', 'pukhpa', '97', 'hominymic', '13', 'neeeeooooowwwww', 'tewific'}\n"
     ]
    }
   ],
   "source": [
    "nunf = set()\n",
    "for s in X:\n",
    "    for w in s:#.split():\n",
    "        if w not in we.stoi:\n",
    "            nunf.add(w)\n",
    "print(len(nunf), nunf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''weX = []\n",
    "for i,s in enumerate(X):\n",
    "    weX.append([])\n",
    "    for w in s.split():\n",
    "        if w in we.stoi:\n",
    "            weX[i].append(get_word_vector(w))'''\n",
    "\n",
    "to_del = []\n",
    "for s in X:\n",
    "    for w in s:\n",
    "        if w not in we.stoi:\n",
    "            to_del.append(w)\n",
    "X = [[w for w in s if w not in to_del] for s in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['so', 'if', 'a', 'photon', 'is', 'directed', 'through', 'a', 'plane', 'with', 'two', 'slits', 'in', 'it', 'and', 'either', 'slit', 'is', 'observed', 'it', 'will', 'not', 'go', 'through', 'both', 'slits', '.'], ['if', 'it', 'unobserved', 'it', 'will', ',', 'however', ',', 'if', 'it', 'observed', 'after', 'it', 'left', 'the', 'plane', 'but', 'before', 'it', 'hits', 'its', 'target', ',', 'it', 'will', 'not', 'have', 'gone', 'through', 'both', 'slits', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(X[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8842\n"
     ]
    }
   ],
   "source": [
    "words_set = set()\n",
    "for s in X:\n",
    "    words_set = words_set.union(set(s))\n",
    "print(len(words_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'set' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-300-769b67fcea4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'set' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "we_idx = [we.stoi[w] for w in list(words_set)]\n",
    "we_idx.append(0) #for padding we need to intialize one row of vector weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-64c5cd7a6aab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0midw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvocab_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'<PAD>'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mvocab_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0midw\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words_set' is not defined"
     ]
    }
   ],
   "source": [
    "# map sentences to vocab\n",
    "idw = 1\n",
    "vocab_X = {'<PAD>':0}\n",
    "for w in list(words_set):\n",
    "    vocab_X[w] = idw\n",
    "    idw += 1\n",
    "\n",
    "# fancy nested list comprehension\n",
    "X_num =  [[vocab_X[word] for word in sentence] for sentence in X]\n",
    "\n",
    "print(len(vocab_X))\n",
    "print(X[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 32]\n",
      "109\n",
      "[[2869.  782. 7686. 2126.  591. 6245. 5823. 7686. 1026. 4200. 8377.  691.\n",
      "  5450. 3236. 7742. 1339. 3146.  591. 4996. 3236.  898. 6747. 3094. 5823.\n",
      "  7503.  691. 2943.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.]\n",
      " [ 782. 3236. 2137. 3236.  898. 7056. 6933. 7056.  782. 3236. 4996. 4470.\n",
      "  3236. 1319. 5097. 1026. 8468. 5910. 3236. 7011. 7127. 5986. 7056. 3236.\n",
      "   898. 6747. 8757. 5292. 5823. 7503.  691. 2943.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.]]\n"
     ]
    }
   ],
   "source": [
    "# get the length of each sentence\n",
    "X_lengths = [len(sentence) for sentence in X_num]\n",
    "print(X_lengths[0:2])\n",
    "\n",
    "# create an empty matrix with padding tokens\n",
    "padding_idx = vocab_X['<PAD>']\n",
    "longest_sent = max(X_lengths)\n",
    "print(longest_sent)\n",
    "batch_size = len(X_num)\n",
    "padded_X = np.ones((batch_size, longest_sent)) * padding_idx\n",
    "\n",
    "# copy over the actual sequences\n",
    "for i, x_len in enumerate(X_lengths):\n",
    "    sequence = X_num[i]\n",
    "    padded_X[i, 0:x_len] = sequence[:x_len]\n",
    "\n",
    "# padded_X looks like:\n",
    "print(padded_X[0:2][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 4, 6],\n",
      "        [1, 3, 5]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'embed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-58e00045c11f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#inp = Variable(torch.LongTensor([3,4]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'embed' is not defined"
     ]
    }
   ],
   "source": [
    "inp = Variable(torch.LongTensor([[3,4,6], [1,3,5]]))\n",
    "#inp = Variable(torch.LongTensor([3,4]))\n",
    "print(inp)\n",
    "embed(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4, 3]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(5,2,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4169, 7342, 5962, 8437, 5164, 7482, 1566, 5962, 2918, 5311, 4970, 2797, 6516, 7058, 2511, 3073, 323, 5164, 8175, 7058, 870, 3717, 6986, 1566, 6629, 2797, 5992], [7342, 7058, 1437, 7058, 870, 6062, 4562, 6062, 7342, 7058, 8175, 7461, 7058, 8475, 129, 2918, 6632, 6473, 7058, 5287, 1649, 2816, 6062, 7058, 870, 3717, 5766, 8358, 1566, 6629, 2797, 5992]]\n",
      "lenght tensor([27, 32,  6,  ...,  8,  2,  2])\n",
      "tensor([[4169, 7342, 5962, 8437, 5164, 7482, 1566, 5962, 2918, 5311, 4970, 2797,\n",
      "         6516, 7058, 2511, 3073,  323, 5164, 8175, 7058,  870, 3717, 6986, 1566,\n",
      "         6629, 2797, 5992,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [7342, 7058, 1437, 7058,  870, 6062, 4562, 6062, 7342, 7058, 8175, 7461,\n",
      "         7058, 8475,  129, 2918, 6632, 6473, 7058, 5287, 1649, 2816, 6062, 7058,\n",
      "          870, 3717, 5766, 8358, 1566, 6629, 2797, 5992,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0]])\n",
      "nb sentences 12702\n",
      "after sum torch.Size([12702, 300])\n",
      "torch.Size([12702, 1, 300])\n",
      "sum tensor([[[-18.5443, -14.0976,  -3.7030,  ...,  25.1799,  -0.4742,   6.1491]],\n",
      "\n",
      "        [[-20.3365, -13.4550,  -6.9550,  ...,  26.5642,   0.2408,   5.9907]],\n",
      "\n",
      "        [[-21.9222, -18.6050,  -0.4607,  ...,  23.6221,  -4.8768,   7.5968]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-22.1710, -19.0927,   0.1030,  ...,  23.4371,  -4.6797,   7.1339]],\n",
      "\n",
      "        [[-23.0833, -18.9567,   0.6230,  ...,  24.1531,  -5.7405,   7.0717]],\n",
      "\n",
      "        [[-23.0383, -19.4225,   0.9062,  ...,  23.9678,  -5.7049,   6.2002]]])\n",
      "torch.Size([1, 600]) torch.Size([4, 1, 300]) torch.Size([4, 1, 1, 300]) torch.Size([1, 300]) torch.Size([1, 300]) torch.Size([1, 1, 600]) tensor(1., dtype=torch.float64)\n",
      "torch.Size([1, 600])\n",
      "torch.Size([1, 1]) torch.Size([1, 600]) torch.Size([1, 600])\n",
      "torch.Size([1, 600])\n",
      "tensor([[1.]], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/people/maurice/anaconda3-5.1/envs/py3.6GPU/lib/python3.6/site-packages/ipykernel_launcher.py:84: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "#https://gist.github.com/Tushar-N/dfca335e370a2bc3bc79876e6270099e\n",
    "\n",
    "def get_last_modulo(nb, mod):\n",
    "    if nb%mod == 0:\n",
    "        return nb\n",
    "    else:\n",
    "        return int(nb/mod) * mod\n",
    "\n",
    "### Premier modèle, somme des embedding par phrases\n",
    "taille_embedding = len(get_word_vector(X[0][0]))\n",
    "taille_context = 3\n",
    "\n",
    "idx_set_words = dict(zip(list(words_set), range(1,len(words_set)+1)))\n",
    "idx_set_words['<PAD>'] = 0\n",
    "padding_idx = idx_set_words['<PAD>']\n",
    "vectorized_seqs = [[idx_set_words[w] for w in s]for s in X]\n",
    "#vectorized_seqs = padded_X\n",
    "print(vectorized_seqs[0:2])\n",
    "embed = nn.Embedding(num_embeddings=len(words_set)+1, embedding_dim=taille_embedding, padding_idx=padding_idx)\n",
    "embed.weight.data.copy_(we.vectors[we_idx])\n",
    "embed.weight.requires_grad = False\n",
    "\n",
    "# get the length of each seq in your batch\n",
    "seq_lengths = torch.LongTensor(list(map(len, vectorized_seqs)))\n",
    "print('length', seq_lengths)\n",
    "# dump padding everywhere, and place seqs on the left.\n",
    "# NOTE: you only need a tensor as big as your longest sequence\n",
    "seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
    "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "    seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "\n",
    "print(seq_tensor[0:2])\n",
    "print('nb sentences', len(vectorized_seqs))\n",
    "\n",
    "# utils.rnn lets you give (B,L,D) tensors where B is the batch size, L is the maxlength, if you use batch_first=True\n",
    "# Otherwise, give (L,B,D) tensors\n",
    "seq_tensor = seq_tensor.transpose(0,1) # (B,L,D) -> (L,B,D)\n",
    "\n",
    "# embed your sequences\n",
    "seq_tensor = embed(seq_tensor)\n",
    "\n",
    "# sum over L, all words per sentence\n",
    "seq_tensor_sumed = torch.sum(seq_tensor, dim=0) #len(vectorized_seqs), taille_embedding\n",
    "print('after sum', seq_tensor_sumed.shape)\n",
    "seq_tensor_sumed = seq_tensor_sumed.view(len(vectorized_seqs), 1, taille_embedding)\n",
    "#seq_tensor_sumed = seq_tensor_sumed.view(seq_tensor_sumed.shape[0], 1, taille_embedding)\n",
    "#seq_tensor_sumed = seq_tensor_sumed.view(int(get_last_modulo(seq_tensor_sumed.shape[0], taille_context)/taille_context), taille_context, seq_tensor_sumed.shape[1])\n",
    "print(seq_tensor_sumed.shape)\n",
    "print('sum', seq_tensor_sumed)\n",
    "\n",
    "bidirectional = False\n",
    "input_size = taille_embedding\n",
    "hidden_size = taille_embedding\n",
    "num_layers = 3\n",
    "nb_sentences = len(vectorized_seqs)\n",
    "lstm_previous = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=False, bidirectional=bidirectional)\n",
    "lstm_future = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=False, bidirectional=bidirectional)\n",
    "tanh = nn.Tanh()\n",
    "softmax = nn.Softmax()\n",
    "#get mini-batch\n",
    "for i in range(nb_sentences - (2*taille_context - 1) -1): #TODO split train/test/val\n",
    "    indices_previous = torch.tensor(list(range(i,i+taille_context+1)))\n",
    "    indices_future = torch.tensor(list(range(i+2*taille_context+1,i+taille_context,-1)))\n",
    "    input_previous_features = torch.index_select(seq_tensor_sumed, 0, indices_previous)\n",
    "    input_future_features = torch.index_select(seq_tensor_sumed, 0, indices_future)\n",
    "    seq_tensor_output_previous, (ht_previous, ct_previous) = lstm_previous(input_previous_features)\n",
    "    seq_tensor_output_future, (ht_future, ct_future) = lstm_future(input_future_features)\n",
    "    target = Y[i + taille_context]\n",
    "    seq_len = input_previous_features.shape[0]\n",
    "    batch = input_previous_features.shape[1]\n",
    "    #print(input_previous_features.shape) # 4 1 300\n",
    "    num_directions = 1\n",
    "    if bidirectional:\n",
    "        num_directions = 2\n",
    "    seq_tensor_output_sum = torch.cat((seq_tensor_output_previous.view(seq_len, batch, num_directions, hidden_size)[-1], seq_tensor_output_future.view(seq_len, batch, num_directions, hidden_size)[-1]), -1)\n",
    "    print(seq_tensor_output_sum[-1].shape, seq_tensor_output_previous.shape, seq_tensor_output_previous.view(seq_len, batch, num_directions, hidden_size).shape, seq_tensor_output_previous[-1].shape, seq_tensor_output_future[-1].shape, seq_tensor_output_sum.shape, target)\n",
    "    #torch.Size([1, 600]) torch.Size([4, 1, 300]) torch.Size([4, 1, 1, 300]) torch.Size([1, 300]) torch.Size([1, 300]) torch.Size([1, 1, 600]) tensor(1., dtype=torch.float64)\n",
    "    #TODO Attention mechanism\n",
    "    seq_tensor_output_sum = seq_tensor_output_sum.view(1,2*num_directions*taille_embedding) #2 is because we concatenate previous and future embeddings\n",
    "    print(seq_tensor_output_sum.shape)\n",
    "    W = torch.rand(1, num_directions)\n",
    "    print(W.shape, seq_tensor_output_sum.shape, W.mm(seq_tensor_output_sum).shape)\n",
    "    print(tanh(W.mm(seq_tensor_output_sum)).shape)\n",
    "    u = torch.rand(1, 2*taille_embedding)\n",
    "    print(softmax(u.mm(tanh(W.mm(seq_tensor_output_sum)).t())))\n",
    "    break\n",
    "# throw them through your LSTM (remember to give batch_first=True here if you packed with it)\n",
    "#lstm = torch.nn.LSTM(input_size=seq_tensor_sumed.shape[2], hidden_size=seq_tensor_sumed.shape[2], num_layers=1, batch_first=False, bidirectional=False)\n",
    "#print(lstm)\n",
    "#seq_tensor_output, (ht, ct) = lstm(seq_tensor_sumed)\n",
    "#print('output', seq_tensor_output)\n",
    "\n",
    "# Or if you just want the final hidden state?\n",
    "#print(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalBiLSTM_on_sentence_embedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, targset_size, num_layers = 3, bidirectional = False):\n",
    "        super(HierarchicalBiLSTM_on_sentence_embedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 1\n",
    "        if self.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm_previous = torch.nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=False, bidirectional=bidirectional)\n",
    "        self.lstm_future = torch.nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=False, bidirectional=bidirectional)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(2*self.num_directions*hidden_dim, targset_size)#2 because we concatenate the both output of the lstm\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(self.num_layers, 1, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, 1, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input_previous_sentences, input_future_sentences):\n",
    "        seq_tensor_output_previous, self.hidden = self.lstm_previous(input_previous_sentences, self.hidden)\n",
    "        seq_tensor_output_future, self.hidden = self.lstm_future(input_future_sentences, self.hidden)\n",
    "        return seq_tensor_output_previous, seq_tensor_output_future\n",
    "        \n",
    "        seq_len = input_previous_sentences.shape[0]\n",
    "        batch = input_previous_sentences.shape[1]\n",
    "\n",
    "        seq_tensor_output_sum = torch.cat((seq_tensor_output_previous.view(seq_len, batch, self.num_directions, self.hidden_dim)[-1,:,:], seq_tensor_output_future.view(seq_len, batch, self.num_directions, self.hidden_dim)[-1,:,:]), -1)\n",
    "        #TODO Attention mechanism\n",
    "        seq_tensor_output_sum = seq_tensor_output_sum.view(batch,2*self.num_directions*self.hidden_dim) #2 is because we concatenate previous and future embeddings\n",
    "\n",
    "        #lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(seq_tensor_output_sum)\n",
    "        tag_space = tag_space[0]\n",
    "        prediction = torch.sigmoid(tag_space)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[922, 4529, 7507, 8269, 5367, 2279, 6276, 7507, 3716, 7294, 2695, 1481, 1339, 8092, 1452, 6333, 7988, 5367, 1563, 8092, 3788, 7709, 3076, 6276, 1788, 1481, 861], [4529, 8092, 3173, 8092, 3788, 4260, 3630, 4260, 4529, 8092, 1563, 4360, 8092, 1857, 4986, 3716, 2342, 2757, 8092, 4039, 3846, 5809, 4260, 8092, 3788, 7709, 2758, 4936, 6276, 1788, 1481, 861]]\n",
      "length tensor([27, 32,  6,  ...,  8,  2,  2])\n",
      "tensor([[ 922, 4529, 7507, 8269, 5367, 2279, 6276, 7507, 3716, 7294, 2695, 1481,\n",
      "         1339, 8092, 1452, 6333, 7988, 5367, 1563, 8092, 3788, 7709, 3076, 6276,\n",
      "         1788, 1481,  861,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [4529, 8092, 3173, 8092, 3788, 4260, 3630, 4260, 4529, 8092, 1563, 4360,\n",
      "         8092, 1857, 4986, 3716, 2342, 2757, 8092, 4039, 3846, 5809, 4260, 8092,\n",
      "         3788, 7709, 2758, 4936, 6276, 1788, 1481,  861,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0]])\n",
      "nb sentences 12702\n",
      "size seq_tensor before torch.Size([109, 12702])\n",
      "size seq_tensor embed torch.Size([109, 12702, 300])\n",
      "after sum torch.Size([12702, 300])\n",
      "torch.Size([12702, 1, 300])\n",
      "sum tensor([[[ -2.4246,   2.7440, -11.8318,  ...,   9.9872,   1.0262,  10.7053]],\n",
      "\n",
      "        [[ -5.6032,   0.4710, -12.4611,  ...,  10.3598,   0.2229,  10.8442]],\n",
      "\n",
      "        [[ -3.6114,  -0.4311, -11.1781,  ...,  10.3936,  -1.0746,  15.1105]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -3.1222,  -0.1930, -11.3503,  ...,   9.1751,  -0.6434,  15.2246]],\n",
      "\n",
      "        [[ -2.6695,  -0.7087, -11.5146,  ...,   9.4781,  -1.3691,  16.0567]],\n",
      "\n",
      "        [[ -2.6468,  -0.3397, -11.7446,  ...,   9.7611,  -1.5683,  16.1776]]])\n",
      "début train\n",
      "ok\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-c365b5d17724>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m#  calling optimizer.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtaille_context\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#targets)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3-5.1/envs/py3.6GPU/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3-5.1/envs/py3.6GPU/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "#https://gist.github.com/Tushar-N/dfca335e370a2bc3bc79876e6270099e\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "### Premier modèle, somme des embedding par phrases\n",
    "taille_embedding = len(get_word_vector(X[0][0]))\n",
    "taille_context = 3\n",
    "bidirectional = False\n",
    "num_layers = 3\n",
    "nb_epoch = 5\n",
    "targset_size = 1\n",
    "\n",
    "idx_set_words = dict(zip(list(words_set), range(1,len(words_set)+1)))\n",
    "idx_set_words['<PAD>'] = 0\n",
    "padding_idx = idx_set_words['<PAD>']\n",
    "vectorized_seqs = [[idx_set_words[w] for w in s]for s in X]\n",
    "#vectorized_seqs = padded_X\n",
    "print(vectorized_seqs[0:2])\n",
    "we_idx = [0] #for padding we need to intialize one row of vector weights\n",
    "we_idx += [we.stoi[w] for w in list(words_set)]\n",
    "embed = nn.Embedding(num_embeddings=len(words_set)+1, embedding_dim=taille_embedding, padding_idx=padding_idx)\n",
    "embed.weight.data.copy_(we.vectors[we_idx])\n",
    "embed.weight.requires_grad = False\n",
    "\n",
    "# get the length of each seq in your batch\n",
    "seq_lengths = torch.LongTensor(list(map(len, vectorized_seqs)))\n",
    "print('length', seq_lengths)\n",
    "# dump padding everywhere, and place seqs on the left.\n",
    "# NOTE: you only need a tensor as big as your longest sequence\n",
    "seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
    "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "    seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "\n",
    "print(seq_tensor[0:2])\n",
    "print('nb sentences', len(vectorized_seqs))\n",
    "\n",
    "# utils.rnn lets you give (B,L,D) tensors where B is the batch size, L is the maxlength, if you use batch_first=True\n",
    "# Otherwise, give (L,B,D) tensors\n",
    "seq_tensor = seq_tensor.transpose(0,1) # (B,L,D) -> (L,B,D)\n",
    "\n",
    "print('size seq_tensor before', seq_tensor.shape)\n",
    "# embed your sequences\n",
    "seq_tensor = embed(seq_tensor)\n",
    "print('size seq_tensor embed', seq_tensor.shape)\n",
    "\n",
    "# sum over L, all words per sentence\n",
    "seq_tensor_sumed = torch.sum(seq_tensor, dim=0) #len(vectorized_seqs), taille_embedding\n",
    "print('after sum', seq_tensor_sumed.shape)\n",
    "seq_tensor_sumed = seq_tensor_sumed.view(len(vectorized_seqs), 1, taille_embedding)\n",
    "#seq_tensor_sumed = seq_tensor_sumed.view(seq_tensor_sumed.shape[0], 1, taille_embedding)\n",
    "#seq_tensor_sumed = seq_tensor_sumed.view(int(get_last_modulo(seq_tensor_sumed.shape[0], taille_context)/taille_context), taille_context, seq_tensor_sumed.shape[1])\n",
    "print(seq_tensor_sumed.shape)\n",
    "print('sum', seq_tensor_sumed)\n",
    "\n",
    "bidirectional = False\n",
    "input_size = taille_embedding\n",
    "hidden_size = taille_embedding\n",
    "num_layers = 3\n",
    "nb_sentences = len(vectorized_seqs)\n",
    "\n",
    "model = HierarchicalBiLSTM_on_sentence_embedding(taille_embedding, taille_embedding, targset_size, num_layers, bidirectional)\n",
    "loss_function = nn.BCELoss()#NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "print('début train')\n",
    "for epoch in range(nb_epoch):\n",
    "    #get mini-batch\n",
    "    #Data loader\n",
    "    for i in range(nb_sentences - (2*taille_context + 1)): #TODO split train/test/val\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        indices_previous = torch.tensor(list(range(i,i+taille_context+1)))\n",
    "        indices_future = torch.tensor(list(range(i+2*taille_context+1,i+taille_context,-1)))\n",
    "        input_previous_features = torch.index_select(seq_tensor_sumed, 0, indices_previous)\n",
    "        input_future_features = torch.index_select(seq_tensor_sumed, 0, indices_future)\n",
    "        \n",
    "        # Step 3. Run our forward pass.\n",
    "        #prediction = model(input_previous_features, input_future_features)\n",
    "        seq_tensor_output_previous, seq_tensor_output_future = model(input_previous_features, input_future_features)\n",
    "        print('ok')\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(prediction, Y[i+taille_context]) #targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #break\n",
    "    #break\n",
    "print('fin train')\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    indices_previous = torch.tensor(list(range(i,i+taille_context+1)))\n",
    "    indices_future = torch.tensor(list(range(i+2*taille_context+1,i+taille_context,-1)))\n",
    "    input_previous_features = torch.index_select(seq_tensor_sumed, 0, indices_previous)\n",
    "    input_future_features = torch.index_select(seq_tensor_sumed, 0, indices_future)\n",
    "    prediction = model(input_previous_features, input_future_features)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(prediction, Y[i+taille_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6674]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    indices_previous = torch.tensor(list(range(i,i+taille_context+1)))\n",
    "    indices_future = torch.tensor(list(range(i+2*taille_context+1,i+taille_context,-1)))\n",
    "    input_previous_features = torch.index_select(seq_tensor_sumed, 0, indices_previous)\n",
    "    input_future_features = torch.index_select(seq_tensor_sumed, 0, indices_future)\n",
    "    prediction = model(input_previous_features, input_future_features)\n",
    "    print(prediction, Y[i+taille_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits.',\n",
       " \"If it's unobserved it will, however, if it's observed after it's left the plane but before it hits its target, it will not have gone through both slits.\",\n",
       " \"Agreed, what's your point?\"]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'if', 'so'}"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(X[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "tensor([[[[ 1.,  2.,  3.,  4.],\n",
      "          [ 5.,  6.,  7.,  8.],\n",
      "          [ 9., 10., 11., 12.]],\n",
      "\n",
      "         [[13., 14., 15., 16.],\n",
      "          [17., 18., 19., 20.],\n",
      "          [21., 22., 23., 24.]]]])\n",
      "tensor([[[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12.],\n",
      "         [13., 14., 15., 16.],\n",
      "         [17., 18., 19., 20.],\n",
      "         [21., 22., 23., 24.]]])\n"
     ]
    }
   ],
   "source": [
    "v = torch.Tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]],[[13,14,15,16],[17,18,19,20],[21,22,23,24]]]) #(1,2,3,4)\n",
    "v = v.unsqueeze(0)\n",
    "print(v.view(1,-1,4).shape[-1])\n",
    "print(v)\n",
    "print(v.view(1,-1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 3, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(torch.randperm(v.size()[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=torch.Tensor([[1,2],[4,5]])\n",
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([109, 12702, 300])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([40, 41, 42, 43]) tensor([47, 46, 45, 44]) tensor([40, 41, 42, 43, 44, 45, 46, 47])\n"
     ]
    }
   ],
   "source": [
    "#indices_previous = torch.tensor(list(range(i,i+taille_context+1)))\n",
    "#indices_future = torch.tensor(list(range(i+2*taille_context+1,i+taille_context,-1)))\n",
    "print(indices_previous, indices_future, torch.tensor(list(range(i,i+2*taille_context+2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([109, 8, 300])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_emb = torch.index_select(seq_tensor, 1, torch.tensor(list(range(i,i+2*taille_context+2))))\n",
    "sentences_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([109, 8, 300])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.LSTM(sentences_emb.shape[2], sentences_emb.shape[2], num_layers=3)\n",
    "h0 = torch.randn(3, sentences_emb.shape[1], sentences_emb.shape[2])\n",
    "c0 = torch.randn(3, sentences_emb.shape[1], sentences_emb.shape[2])\n",
    "output, (hn, cn) = rnn(sentences_emb, (h0, c0))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 300])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-1,:,:].unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 300])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-1,:,:].unsqueeze(1)[int(sentences_emb.shape[1]/2):,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12702, 1, 300])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor_sumed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 300])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_previous_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 300]) torch.Size([4, 1, 300])\n",
      "torch.Size([1, 600])\n",
      "torch.Size([10, 1])\n",
      "torch.Size([1, 1])\n",
      "tensor([[-0.0233]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(seq_tensor_output_previous.shape, seq_tensor_output_future.shape)\n",
    "similarity = torch.cat((seq_tensor_output_previous[-1,:,:], seq_tensor_output_future[0,:,:]), 1)\n",
    "print(similarity.shape)\n",
    "attn = nn.Linear(similarity.shape[1],10)\n",
    "tanh = nn.Tanh()\n",
    "similarity_ = tanh(attn(similarity)).transpose(0,1)\n",
    "print(similarity_.shape)\n",
    "#seq_tensor_output_previous[-1,:,:].matmul(seq_tensor_output_future[0,:,:])\n",
    "#alpha = nn.Linear(similarity_,1)\n",
    "u = torch.empty(1, similarity_.shape[0]).uniform_(0, 1)\n",
    "alpha = u.matmul(similarity_)\n",
    "print(alpha.shape)\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0540,  0.0462,  0.0029,  ...,  0.0238,  0.0034,  0.0120]],\n",
       "\n",
       "        [[ 0.0540,  0.0462,  0.0029,  ...,  0.0238,  0.0034,  0.0120]],\n",
       "\n",
       "        [[ 0.0540,  0.0462,  0.0029,  ...,  0.0238,  0.0034,  0.0120]],\n",
       "\n",
       "        [[ 0.0540,  0.0462,  0.0029,  ...,  0.0238,  0.0034,  0.0120]]],\n",
       "       grad_fn=<RepeatBackward>)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor_output_previous[-1,:,:].repeat(seq_tensor_output_future.shape[0],1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 300]) torch.Size([4, 1, 300])\n",
      "torch.Size([4, 1, 600])\n",
      "torch.Size([4, 1, 10])\n",
      "torch.Size([4, 1, 1])\n",
      "tensor([[[0.1699]],\n",
      "\n",
      "        [[0.1685]],\n",
      "\n",
      "        [[0.1687]],\n",
      "\n",
      "        [[0.1693]]], grad_fn=<ThAddBackward>)\n",
      "torch.Size([1, 1, 4])\n",
      "tensor([[[0.2502, 0.2499, 0.2499, 0.2501]]], grad_fn=<TransposeBackward0>)\n",
      "torch.Size([1, 4, 300])\n",
      "torch.Size([1, 1, 300])\n",
      "tensor([[[ 0.0262,  0.0044, -0.0322, -0.0046,  0.0071,  0.0323, -0.0168,\n",
      "           0.0076,  0.0247, -0.0462, -0.0135, -0.0454, -0.0468, -0.0304,\n",
      "           0.0196, -0.0172, -0.0252,  0.0043, -0.0297, -0.0191,  0.0021,\n",
      "          -0.0182,  0.0505,  0.0280, -0.0208, -0.0050, -0.0235, -0.0276,\n",
      "          -0.0181,  0.0103, -0.0447, -0.0253,  0.0337, -0.0136, -0.0326,\n",
      "           0.0333,  0.0374, -0.0353,  0.0186, -0.0264,  0.0330,  0.0021,\n",
      "          -0.0179, -0.0116,  0.0098, -0.0015,  0.0123, -0.0356,  0.0067,\n",
      "           0.0599, -0.0083, -0.0187, -0.0078,  0.0005, -0.0059, -0.0521,\n",
      "           0.0194,  0.0005,  0.0225, -0.0084,  0.0504,  0.0090,  0.0237,\n",
      "           0.0188, -0.0002,  0.0028, -0.0035, -0.0209, -0.0458, -0.0507,\n",
      "          -0.0384,  0.0488,  0.0405, -0.0451, -0.0291,  0.0104, -0.0147,\n",
      "           0.0652,  0.0009, -0.0208,  0.0207, -0.0056,  0.0182,  0.0054,\n",
      "          -0.0447, -0.0167, -0.0585,  0.0043, -0.0156, -0.0083,  0.0270,\n",
      "          -0.0070, -0.0321,  0.0381,  0.0623,  0.0064, -0.0577,  0.0179,\n",
      "          -0.0683,  0.0173,  0.0183, -0.0105, -0.0119,  0.0502, -0.0054,\n",
      "          -0.0623, -0.0630,  0.0013,  0.0038,  0.0135, -0.0397,  0.0062,\n",
      "           0.0023,  0.0198, -0.0003,  0.0037,  0.0135, -0.0069, -0.0083,\n",
      "          -0.0485,  0.0182,  0.0196,  0.0068, -0.0024,  0.0299, -0.0193,\n",
      "           0.0137, -0.0016, -0.0087, -0.0330, -0.0291, -0.0207, -0.0133,\n",
      "           0.0383, -0.0462, -0.0183, -0.0195, -0.0064, -0.0161, -0.0145,\n",
      "          -0.0299,  0.0539, -0.0012, -0.0475, -0.0123, -0.0071,  0.0484,\n",
      "          -0.0433,  0.0411,  0.0275, -0.0508,  0.0410,  0.0512,  0.0231,\n",
      "          -0.0083,  0.0291, -0.0244,  0.0394,  0.0346,  0.0106,  0.0570,\n",
      "           0.0151,  0.0209, -0.0068,  0.0348, -0.0068, -0.0012, -0.0099,\n",
      "          -0.0034, -0.0308,  0.0068, -0.0165,  0.0012, -0.0064,  0.0263,\n",
      "          -0.0596,  0.0297,  0.0021, -0.0278,  0.0139,  0.0434, -0.0062,\n",
      "           0.0350,  0.0331,  0.0027,  0.0119,  0.0183, -0.0152, -0.0130,\n",
      "           0.0030,  0.0460,  0.0299,  0.0166,  0.0267,  0.0391, -0.0149,\n",
      "           0.0570,  0.0026, -0.0331,  0.0209,  0.0091, -0.0058, -0.0218,\n",
      "           0.0176,  0.0532, -0.0143,  0.0045, -0.0195, -0.0326,  0.0373,\n",
      "          -0.0071, -0.0150, -0.0290, -0.0277,  0.0437,  0.0021,  0.0035,\n",
      "          -0.0031,  0.0059, -0.0012,  0.0202,  0.0155, -0.0098,  0.0066,\n",
      "           0.0533,  0.0647, -0.0281,  0.0094, -0.0298, -0.0128,  0.0294,\n",
      "           0.0231,  0.0110, -0.0429, -0.0363,  0.0411,  0.0183,  0.0173,\n",
      "           0.0095, -0.0149,  0.0298,  0.0197,  0.0463, -0.0147, -0.0339,\n",
      "          -0.0903,  0.0157, -0.0142, -0.0706, -0.0293, -0.0099, -0.0227,\n",
      "           0.0465,  0.0278, -0.0119, -0.0648,  0.0241, -0.0128,  0.0316,\n",
      "           0.0585, -0.0214,  0.0414, -0.0007, -0.0432, -0.0018,  0.0360,\n",
      "          -0.0470, -0.0203, -0.0036,  0.0017, -0.0680, -0.0129, -0.0052,\n",
      "           0.0413,  0.0181, -0.0453, -0.0122,  0.0357,  0.0471,  0.0510,\n",
      "           0.0288,  0.0238, -0.0053, -0.0283, -0.0128, -0.0291,  0.0351,\n",
      "           0.0330,  0.0012,  0.0415, -0.0706, -0.0318,  0.0300,  0.0049,\n",
      "          -0.0099,  0.0146, -0.0087,  0.0378,  0.0594, -0.0419]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(seq_tensor_output_previous.shape, seq_tensor_output_future.shape)\n",
    "similarity = torch.cat((seq_tensor_output_previous[-1,:,:].repeat(seq_tensor_output_future.shape[0],1,1), seq_tensor_output_future[:,:,:]), 2)\n",
    "print(similarity.shape)\n",
    "attn = nn.Linear(similarity.shape[2],10)\n",
    "tanh = nn.Tanh()\n",
    "similarity_ = tanh(attn(similarity))\n",
    "print(similarity_.shape)\n",
    "#seq_tensor_output_previous[-1,:,:].matmul(seq_tensor_output_future[0,:,:])\n",
    "alpha = nn.Linear(similarity_.shape[2],1)\n",
    "alpha_ = alpha(similarity_)\n",
    "print(alpha_.shape)\n",
    "print(alpha_)\n",
    "softmax = nn.Softmax(dim=0)\n",
    "alpha_ = softmax(alpha_).transpose(0,1).transpose(1,2)#.unsqueeze(0)\n",
    "print(alpha_.shape)\n",
    "print(alpha_)\n",
    "print(seq_tensor_output_future.transpose(0,1).shape)\n",
    "m_p = torch.matmul(alpha_, seq_tensor_output_future.transpose(0,1)).transpose(0,1) #(32,1,4) * (32,4,4096)\n",
    "print(m_p.shape)\n",
    "print(m_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1200])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-228-a190f72597a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mseq_tensor_output_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_tensor_output_previous\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_tensor_output_future\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_tensor_output_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mseq_tensor_output_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_tensor_output_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_directions\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "seq_tensor_output_sum = torch.cat((seq_tensor_output_previous[-1,:,:], seq_tensor_output_future[-1,:,:], m_p, m_p), -1)\n",
    "print(seq_tensor_output_sum.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4335, -1.0794,  0.0333,  2.1552])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.5426, 2.5426, 2.5426],\n",
       "        [2.5426, 2.5426, 2.5426],\n",
       "        [2.5426, 2.5426, 2.5426],\n",
       "        [2.5426, 2.5426, 2.5426],\n",
       "        [2.5426, 2.5426, 2.5426],\n",
       "        [2.5426, 2.5426, 2.5426],\n",
       "        [2.5426, 2.5426, 2.5426],\n",
       "        [2.5426, 2.5426, 2.5426],\n",
       "        [2.5426, 2.5426, 2.5426],\n",
       "        [2.5426, 2.5426, 2.5426]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.ones(10, 3, 4)\n",
    "tensor2 = torch.randn(4)\n",
    "print(tensor2)\n",
    "torch.matmul(tensor1, tensor2)#.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits.', \"If it's unobserved it will, however, if it's observed after it's left the plane but before it hits its target, it will not have gone through both slits.\", \"Agreed, what's your point?\", \"There's no point, I just think it's a good idea for a tee-shirt. \", 'Excuse me?', 'Hang on. ', 'One across is Aegean, eight down is Nabakov, twenty-six across is MCM, fourteen down is... move your finger... phylum, which makes fourteen across Port-au-Prince.', \"See, Papa Doc's capital idea, that's Port-au-Prince.\", 'Haiti. ', 'Can I help you?', 'Yes.', 'Um, is this the High IQ sperm bank?', \"If you have to ask, maybe you shouldn't be here.\", 'I think this is the place.', 'Fill these out.', 'Thank-you.', \"We'll be right back.\", 'Oh, take your time.', \"I'll just finish my crossword puzzle.\", 'Oh wait.', \"Leonard, I don't think I can do this.\", 'What, are you kidding?', \"You're a semi-pro. \", 'No.', 'We are committing genetic fraud.', \"There's no guarantee that our sperm is going to generate high IQ offspring, think about that.\", 'I have a sister with the same basic DNA mix who hostesses at Fuddruckers.', 'Sheldon, this was your idea.', 'A little extra money to get fractional T1 bandwidth in the apartment.', \"I know, and I do yearn for faster downloads, but there's some poor woman is going to pin her hopes on my sperm, what if she winds up with a toddler who doesn't know if he should use an integral or a differential to solve the area under a curve.\", \"I'm sure she'll still love him.\", \"I wouldn't.\", 'Well, what do you want to do?', 'I want to leave.', 'Okay.', \"What's the protocol for leaving?\", \"I don't know, I've never reneged on a proffer of sperm before.\", \"Let's try just walking out.\", 'Okay.', 'Bye.', 'Bye-bye', 'See you.', 'Are you still mad about the sperm bank?', 'No.', 'You want to hear an interesting thing about stairs?', 'Not really.', 'If the height of a single step is off by as little as two millimetres, most people will trip.', \"I don't care.\", 'Two millimetres?', \"That doesn't seem right.\", \"No, it's true, I did a series of experiments when I was twelve, my father broke his clavicle.\", 'Is that why they sent you to boarding school?', 'No, that was the result of my work with lasers.', 'New neighbor?', 'Evidently.', 'Significant improvement over the old neighbor.', 'Two hundred pound transvestite with a skin condition, yes she is.', 'Oh, hi!', 'Hi.', 'Hi.', 'Hi.', 'Hi.', 'Hi?', \"We don't mean to interrupt, we live across the hall.\", \"Oh, that's nice.\", \"Oh... uh... no... we don't live together... um... we live together but in separate, heterosexual bedrooms.\", \"Oh, okay, well, guess I'm your new neighbor, Penny.\", 'Leonard, Sheldon.', 'Hi.', 'Hi.', 'Hi.', 'Hi. ', 'Hi.', 'Well, uh, oh, welcome to the building.', 'Thankyou, maybe we can have coffee sometime.', 'Oh, great.', 'Great. ', 'Great.', 'Great.', 'Well, bye.', 'Bye.', 'Bye.', 'Bye. ', 'Should we have invited her for lunch?', 'No.', \"We're going to start Season Two of Battlestar Galactica.\", 'We already watched the Season Two DVDs.', 'Not with commentary.', 'I think we should be good neighbors, invite her over, make her feel welcome.', 'We never invited Louis-slash-Louise over.', 'Well, then that was wrong of us.', 'We need to widen our circle.', 'I have a very wide circle.', 'I have 212 friends on myspace. ', \"Yes, and you've never met one of them.\", \"That's the beauty of it.\", \"I'm going to invite her over.\", \"We'll have a nice meal and chat.\", 'Chat?', \"We don't chat.\", 'At least not offline.', \"Well it's not difficult, you just listen to what she says and then you say something appropriate in response.\", 'To what end?', 'Hi.', 'Again.', 'Hi.', 'Hi.', 'Hi.', 'Hi. ', 'Anyway, um.', 'We brought home Indian food.', 'And, um.', \"I know that moving can be stressful, and I find that when I'm undergoing stress, that good food and company can have a comforting effect.\", \"Also, curry is a natural laxative, and I don't have to tell you that, uh, a clean colon is just one less thing to worry about.\", \"Leonard, I'm not expert here but I believe in the context of a luncheon invitation, you might want to skip the reference to bowel movements.\", \"Oh, you're inviting me over to eat?\", 'Uh, yes.', \"Oh, that's so nice, I'd love to.\", 'Great.', 'So, what do you guys do for fun around here?', 'Well, today we tried masturbating for money.', 'Okay, well, make yourself at home.', 'Okay, thankyou.', \"You're very welcome.\", 'This looks like some serious stuff, Leonard, did you do this?', \"Actually that's my work.\", 'Wow.', \"Yeah, well, it's just some quantum mechanics, with a little string theory doodling around the edges.\", \"That part there, that's just a joke, it's a spoof of the Bourne-Oppenheimer approximation.\", \"So you're like, one of those, beautiful mind genius guys.\", 'Yeah. ', 'This is really impressive.', 'I have a board.', 'If you like boards, this is my board.', 'Holy smokes.', \"If by holy smokes you mean a derivative restatement of the kind of stuff you can find scribbled on the wall of any men's room at MIT, sure.\", 'What?', 'Oh, come on.', \"Who hasn't seen this differential below 'here I sit broken hearted?'\", \"At least I didn't have to invent twenty-six dimensions just to make the math come out.\", \"I didn't invent them, they're there.\", 'In what universe?', 'In all of them, that is the point.', 'Uh, do you guys mind if I start?', \"Um, Penny, that's where I sit.\", 'So, sit next to me. ', 'No, I sit there.', \"What's the difference?\", \"What's the difference?\", 'Here we go.', 'In the winter that seat is close enough to the radiator to remain warm, and yet not so close as to cause perspiration.', \"In the summer it's directly in the path of a cross breeze created by open windows there, and there.\", \"It faces the television at an angle that is neither direct, thus discouraging conversation, nor so far wide to create a parallax distortion, I could go on, but I think I've made my point. \", 'Do you want me to move?', 'Well.', 'Just sit somewhere else.', 'Fine. ', 'Sheldon, sit!', 'Aaah!', 'Well, this is nice.', \"We don't have a lot of company over.\", \"That's not true.\", 'Koothrapali and Wolowitz come over all the time. ', 'Yes I know, but...', 'Tuesday night we played Klingon boggle until one in the morning.', 'Yes, I remember.', \"I resent you saying we don't have company.\", \"I'm sorry.\", 'That is an antisocial implication.', \"I said I'm sorry.\", 'So, Klingon boggle?', \"Yeah, it's like regular boggle but, in Klingon.\", \"That's probably enough about us, tell us about you.\", \"Um, me, okay, I'm Sagittarius, which probably tells you way more than you need to know.\", \"Yes, it tells us that you participate in the mass cultural delusion that the Sun's apparent position relative to arbitrarily defined constellations and the time of your birth somehow effects your personality.\", 'Participate in the what?', \"I think what Sheldon's trying to say, is that Sagittarius wouldn't have been our first guess.\", \"Oh, yeah, a lot of people think I'm a water sign.\", \"Okay, let's see, what else, oh, I'm a vegetarian, oh, except for fish, and the occasional steak, I love steak. \", \"That's interesting.\", \"Leonard can't process corn.\", 'Wu-uh, do you have some sort of a job?', \"Oh, yeah, I'm a waitress at the Cheesecake Factory.\", 'Oh, okay.', 'I love cheesecake.', \"You're lactose intolerant. \", \"I don't eat it, I just think it's a good idea.\", \"Oh, anyways, I'm also writing a screenplay.\", \"It's about this sensitive girl who comes to L.A from Lincoln Nebraska to be an actress, and winds up a waitress at the Cheesecake Factory.\", \"So it's based on your life?\", \"No, I'm from Omaha. \", 'Well, if that was a movie I would go see it.', 'I know, right?', \"Okay, let's see, what else?\", \"Um, that's about it.\", \"That's the story of Penny.\", 'Well it sounds wonderful.', 'It was.', 'Until I fell in love with a jerk. ', \"What's happening.\", \"I don't know.\", \"Oh God, you know, four years I lived with him, four years, that's like as long as High School. \", 'It took you four years to get through High School?', \"Don't.\", \"I just, I can't believe I trusted him.\", 'Should I say something?', 'I feel like I should say something.', 'You?', \"No, you'll only make it worse.\", 'You want to know the most pathetic part?', 'Even though I hate his lying, cheating guts, I still love him.', 'Is that crazy?', 'Yes. ', \"No, it's not crazy it's, uh, uh, it's a paradox.\", 'And paradoxes are part of nature, think about light.', 'Now if you look at Huygens, light is a wave, as confirmed by the double slit experiments, but then, along comes Albert Einstein and discovers that light behaves like particles too.', \"Well, I didn't make it worse.\", \"Oh, I'm so sorry, I'm such a mess, and on top of everything else I'm all gross from moving and my stupid shower doesn't even work.\", 'Our shower works.', 'Really?', 'Would it be totally weird if I used it?', 'Yes. ', 'No.', 'No?', 'No.', 'No.', \"It's right down the hall.\", 'Thanks.', 'You guys are really sweet.', 'Well, this is an interesting development. ', 'How so?', \"It has been some time since we've had a woman take her clothes off in our apartment.\", \"That's not true, remember at Thanksgiving my grandmother with Alzheimer's had that episode.\", 'Point taken.', \"It has been some time since we've had a woman take her clothes off after which we didn't want to rip our eyes out. \", 'The worst part was watching her carve that turkey.', 'So, what exactly are you trying to accomplish here?', 'Excuse me?', \"That woman in there's not going to have sex with you.\", \"Well I'm not trying to have sex with her.\", 'Oh, good.', \"Then you won't be disappointed.\", \"What makes you think she wouldn't have sex with me, I'm a male and she's a female?\", 'Yes, but not of the same species.', \"I'm not going to engage in hypotheticals here, I'm just trying to be a good neighbor.\", 'Oh, of course.', \"That's not to say that if a carnal relationship were to develop that I wouldn't participate.\", 'However briefly.', 'Do you think this possibility will be helped or hindered when she discovers your Luke Skywalker no-more-tears shampoo?', \"It's Darth Vader shampoo.Luke\", \"Skywalker's the conditioner.\", 'Wait till you see this.', \"It's fantastic.\", 'Unbelievable.', 'See what?', \"It's a Stephen Hawking lecture from MIT in 1974.\", 'This is not a good time.', \"It's before he became a creepy computer voice.\", \"That's great, you guys have to go.\", 'Why?', \"It's just not a good time.\", 'Leonard has a lady over.', 'Yeah, right, your grandmother back in town?', 'No.', \"And she's not a lady, she's just a new neighbor.\", 'Hang on, there really is a lady here?', 'Uh-huh.', \"And you want us out because you're anticipating coitus?\", \"I'm not anticipating coitus.\", \"So she's available for coitus?\", 'Can we please stop saying coitus?', 'Technically that would be coitus interruptus.', 'Hey, is there a trick to getting it to switch from tub to shower?', 'Oh.', 'Hi, sorry.', 'Hello!', 'Enchante Madamoiselle.', 'Howard Wolowitz, Cal-Tech department of Applied Physics.', \"You may be familiar with some of my work, it's currently orbiting Jupiter's largest moon taking high-resolution digital photographs.\", 'Penny.', 'I work at the Cheesecake Factory.', \"Come on, I'll show you the trick with the shower.\", 'Bon douche.', \"I'm sorry?\", \"It's French for good shower.\", \"It's a sentiment I can express in six languages.\", 'Save it for your blog, Howard.', 'See-ka-tong-guay-jow.', \"Uh, there it goes, it sticks, I'm sorry.\", 'Okay.', 'Thanks. ', \"You're welcome, oh, you're going to step right, okay, I'll...\", 'Hey, Leonard?', \"The hair products are Sheldon's.\", 'Um, okay.', 'Can I ask you a favour.', 'A favour?', 'Sure, you can ask me a favour, I would do you a favour for you.', \"It's okay if you say no.\", \"Oh, I'll probably say yes.\", \"It's just not the kind of thing you ask a guy you've just met.\", 'Wow.', 'I really think we should examine the chain of causality here.', 'Must we?', 'Event A.', 'A beautiful woman stands naked in our shower.', 'Event B.', \"We drive half way across town to retrieve a television set from the aforementioned woman's ex-boyfriend.\", 'Query, on what plane of existence is there even a semi-rational link between these events?', 'She asked me to do her a favour, Sheldon.', 'Ah, yes, well that may be the proximal cause of our journey, but we both know it only exists in contradistinction to the higher level distal cause.', 'Which is?', 'You think with your penis.', \"That's a biological impossibility and you didn't have to come.\", 'Oh, right, yes, I could have stayed behind and watched Wolowitz try to hit on Penny in Russian, Arabic and Farsi.', \"Why can't she get her own TV.\", 'Come on, you know how it is with break-ups.', \"No I don't.\", 'And neither do you.', 'Wuh, I, I broke up with Joyce Kim.', 'You did not break up with Joyce Kim, she defected to North Korea.', 'To mend her broken heart.', 'This situation is much less complicated.', \"There's some kind of dispute between Penny and her ex-boyfriend as to who gets custody of the TV.\", 'She just wanted to avoid having a scene with him.', 'So we get to have a scene with him?', \"No, Sheldon, there's not going to be a scene.\", \"There's two of us and one of him.\", \"Leonard, the two of us can't even carry a TV. \", 'So, you guys work with Leonard and Sheldon at the University?', \"Uh, I'm sorry, do you speak English?\", \"Oh, he speaks English, he just can't speak to women.\", 'Really, why?', \"He's kind of a nerd.\", 'Juice box?', \"I'll do the talking.\", 'Yeah.', \"Hi, I'm Leonard, this is Sheldon.\", 'Hello.', 'What did I just...', \"Uh, we're here to pick up Penny's TV.\", 'Get lost.', 'Okay, thanks for your time.', \"We're not going to give up just like that.\", \"Leonard, the TV is in the building, we've been denied access to the building, ergo we are done.\", 'Excuse me, if I were to give up at the first little hitch I never would have been able to identify the fingerprints of string theory in the aftermath of the big bang.', 'My apologies.', \"What's your plan.\", \"It's just a privilege to watch your mind at work.\", 'Come on, we have a combined IQ of 360, we should be able to figure out how to get into a stupid building.', 'What do you think their combined IQ is?', 'Just grab the door.', 'This is it.', \"I'll do the talking.\", \"Good thinking, I'll just be the muscle.\", 'Yeah?', \"I'm Leonard, this is Sheldon.\", 'From the intercom.', 'How the hell did you get in the building?', 'Oh.', \"We're scientists.\", 'Tell him about our IQ.', 'Leonard.', 'What?', 'My mom bought me those pants.', \"I'm sorry.\", \"You're going to have to call her.\", \"Sheldon, I'm so sorry I dragged you through this.\", \"It's okay.\", \"It wasn't my first pantsing, and it won't be my last.\", 'And you were right about my motives, I was hoping to establish a relationship with Penny that might have some day led to sex.', 'Well you got me out of my pants.', \"Anyway, I've learned my lesson.\", \"She's out of my league, I'm done with her, I've got my work, one day I'll win the Nobel Prize and then I'll die alone.\", \"Don't think like that, you're not going to die alone.\", \"Thank you Sheldon, you're a good friend.\", \"And you're certainly not going to win a Nobel Prize.\", 'This is one of my favourite places to kick back after a quest, they have a great house ale.', 'Wow, cool tiger.', \"Yeah, I've had him since level ten.\", 'His name is Buttons.', 'Anyway, if you had your own game character we could hang out, maybe go on a quest.', 'Uh, sounds interesting.', \"So you'll think about it?\", \"Oh, I don't think I'll be able to stop thinking about it.\", 'Smooth.', \"We're home.\", 'Oh, my God, what happened?', 'Well, your ex-boyfriend sends his regards and I think the rest is fairly self-explanatory.', \"I'm so sorry, I really thought if you guys went instead of me he wouldn't be such an ass.\", 'No, it was a valid hypothesis.', 'That was a valid hypothesis?', 'What is happening to you?', \"Really, thank you so much for going and trying you're, uh, you're so terrific.\", \"Why don't you put some clothes on, I'll get my purse and dinner is on me, okay?\", 'Really?', 'Great.', 'Thank you.', \"You're not done with her, are you?\", 'Our babies will be smart and beautiful.', 'Not to mention imaginary.', 'Is Thai food okay with you Penny?', 'Sure.', \"We can't have Thai food, we had Indian for lunch.\", 'So?', \"They're both curry based cuisines.\", 'So?', 'They would be gastronomically redundant.', \"I can see we're going to have to spell out everything for this girl.\", 'Any ideas Raj? ', 'Turn left on Lake Street and head up to Colorado.', 'I know a wonderful little sushi bar that has karaoke.', 'That sounds like fun.', \"Baby, baby don't get hooked on me.\", \"Uh, baby, baby don't get hooked on me.\", \"I don't know what your odds are in the world as a whole, but as far as the population of this car goes, you're a veritable Mack Daddy.\"]\n",
      "['sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'receptionist', 'leonard', 'leonard', 'leonard', 'receptionist', 'leonard', 'leonard', 'receptionist', 'sheldon', 'receptionist', 'leonard', 'leonard', 'receptionist', 'receptionist', 'receptionist', 'sheldon', 'leonard', 'leonard', 'sheldon', 'sheldon', 'sheldon', 'sheldon', 'leonard', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'receptionist', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'leonard', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'penny', 'leonard', 'sheldon', 'leonard', 'sheldon', 'penny', 'leonard', 'penny', 'leonard', 'penny', 'leonard', 'penny', 'leonard', 'sheldon', 'penny', 'leonard', 'leonard', 'penny', 'leonard', 'penny', 'sheldon', 'leonard', 'leonard', 'penny', 'sheldon', 'leonard', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'leonard', 'sheldon', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'leonard', 'penny', 'sheldon', 'leonard', 'penny', 'leonard', 'leonard', 'leonard', 'leonard', 'leonard', 'sheldon', 'penny', 'leonard', 'penny', 'leonard', 'penny', 'sheldon', 'leonard', 'penny', 'leonard', 'penny', 'sheldon', 'penny', 'sheldon', 'sheldon', 'penny', 'sheldon', 'penny', 'leonard', 'leonard', 'penny', 'sheldon', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'penny', 'sheldon', 'penny', 'sheldon', 'penny', 'sheldon', 'leonard', 'sheldon', 'sheldon', 'sheldon', 'penny', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'penny', 'leonard', 'leonard', 'penny', 'sheldon', 'penny', 'leonard', 'penny', 'penny', 'sheldon', 'sheldon', 'leonard', 'penny', 'leonard', 'leonard', 'sheldon', 'leonard', 'penny', 'penny', 'leonard', 'penny', 'leonard', 'penny', 'penny', 'penny', 'penny', 'leonard', 'penny', 'penny', 'sheldon', 'leonard', 'penny', 'sheldon', 'leonard', 'penny', 'leonard', 'leonard', 'sheldon', 'sheldon', 'penny', 'penny', 'penny', 'sheldon', 'leonard', 'leonard', 'leonard', 'leonard', 'penny', 'leonard', 'penny', 'penny', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'penny', 'penny', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'leonard', 'sheldon', 'leonard', 'leonard', 'howard', 'raj', 'raj', 'leonard', 'howard', 'leonard', 'howard', 'leonard', 'raj', 'leonard', 'sheldon', 'howard', 'leonard', 'leonard', 'howard', 'leonard', 'howard', 'leonard', 'howard', 'leonard', 'sheldon', 'penny', 'penny', 'penny', 'penny', 'howard', 'howard', 'howard', 'penny', 'penny', 'leonard', 'howard', 'penny', 'howard', 'howard', 'leonard', 'howard', 'leonard', 'penny', 'penny', 'leonard', 'penny', 'leonard', 'penny', 'penny', 'leonard', 'leonard', 'penny', 'leonard', 'penny', 'leonard', 'sheldon', 'leonard', 'sheldon', 'sheldon', 'sheldon', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'leonard', 'leonard', 'leonard', 'sheldon', 'leonard', 'leonard', 'sheldon', 'penny', 'penny', 'howard', 'penny', 'howard', 'howard', 'leonard', 'voice_from_buzzer', 'leonard', 'sheldon', 'leonard', 'leonard', 'voice', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'leonard', 'leonard', 'sheldon', 'enormous_man', 'leonard', 'sheldon', 'man', 'leonard', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'leonard', 'sheldon', 'leonard', 'sheldon', 'howard', 'penny', 'howard', 'howard', 'howard', 'penny', 'howard', 'penny', 'raj', 'leonard', 'penny', 'leonard', 'penny', 'leonard', 'sheldon', 'sheldon', 'penny', 'penny', 'leonard', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'penny', 'sheldon', 'penny', 'sheldon', 'penny', 'sheldon', 'sheldon', 'penny', 'howard', 'howard', 'penny', 'howard', 'howard', 'sheldon']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "\n",
    "punctuations_end_sentence = ['.', '?', '!']\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for f in sorted(glob.glob('/vol/work2/galmant/transcripts/*')):\n",
    "    with open(f, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "        X_ = []\n",
    "        Y_ = []\n",
    "        for row in reader:\n",
    "            #print(row[1], ' '.join(row[2:]))\n",
    "            #print(row[2:])\n",
    "            sentence = row[2]\n",
    "            old_word = row[2]\n",
    "            for word in row[3:]:\n",
    "                #print(word, sentence, old_word)\n",
    "                #print(word, any(punctuation in old_word for punctuation in punctuations_end_sentence), word[0].isupper())\n",
    "                if any(punctuation in old_word for punctuation in punctuations_end_sentence) and word and word[0].isupper():\n",
    "                    X_.append(sentence)\n",
    "                    Y_.append(row[1])\n",
    "                    sentence = word\n",
    "                else:\n",
    "                    sentence += ' '+word\n",
    "                old_word = word\n",
    "            X_.append(sentence)\n",
    "            Y_.append(row[1])\n",
    "        X.append(X_)\n",
    "        Y.append(Y_)\n",
    "    break\n",
    "print(X[0])\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits.', \"If it's unobserved it will, however, if it's observed after it's left the plane but before it hits its target, it will not have gone through both slits.\", \"Agreed, what's your point?\", \"There's no point, I just think it's a good idea for a tee-shirt. \", 'Excuse me?', 'Hang on. ', 'One across is Aegean, eight down is Nabakov, twenty-six across is MCM, fourteen down is... move your finger... phylum, which makes fourteen across Port-au-Prince.', \"See, Papa Doc's capital idea, that's Port-au-Prince.\", 'Haiti. ', 'Can I help you?', 'Yes.', 'Um, is this the High IQ sperm bank?', \"If you have to ask, maybe you shouldn't be here.\", 'I think this is the place.', 'Fill these out.', 'Thank-you.', \"We'll be right back.\", 'Oh, take your time.', \"I'll just finish my crossword puzzle.\", 'Oh wait.', \"Leonard, I don't think I can do this.\", 'What, are you kidding?', \"You're a semi-pro. \", 'No.', 'We are committing genetic fraud.', \"There's no guarantee that our sperm is going to generate high IQ offspring, think about that.\", 'I have a sister with the same basic DNA mix who hostesses at Fuddruckers.', 'Sheldon, this was your idea.', 'A little extra money to get fractional T1 bandwidth in the apartment.', \"I know, and I do yearn for faster downloads, but there's some poor woman is going to pin her hopes on my sperm, what if she winds up with a toddler who doesn't know if he should use an integral or a differential to solve the area under a curve.\", \"I'm sure she'll still love him.\", \"I wouldn't.\", 'Well, what do you want to do?', 'I want to leave.', 'Okay.', \"What's the protocol for leaving?\", \"I don't know, I've never reneged on a proffer of sperm before.\", \"Let's try just walking out.\", 'Okay.', 'Bye.', 'Bye-bye', 'See you.', 'Are you still mad about the sperm bank?', 'No.', 'You want to hear an interesting thing about stairs?', 'Not really.', 'If the height of a single step is off by as little as two millimetres, most people will trip.', \"I don't care.\", 'Two millimetres?', \"That doesn't seem right.\", \"No, it's true, I did a series of experiments when I was twelve, my father broke his clavicle.\", 'Is that why they sent you to boarding school?', 'No, that was the result of my work with lasers.', 'New neighbor?', 'Evidently.', 'Significant improvement over the old neighbor.', 'Two hundred pound transvestite with a skin condition, yes she is.', 'Oh, hi!', 'Hi.', 'Hi.', 'Hi.', 'Hi.', 'Hi?', \"We don't mean to interrupt, we live across the hall.\", \"Oh, that's nice.\", \"Oh... uh... no... we don't live together... um... we live together but in separate, heterosexual bedrooms.\", \"Oh, okay, well, guess I'm your new neighbor, Penny.\", 'Leonard, Sheldon.', 'Hi.', 'Hi.', 'Hi.', 'Hi. ', 'Hi.', 'Well, uh, oh, welcome to the building.', 'Thankyou, maybe we can have coffee sometime.', 'Oh, great.', 'Great. ', 'Great.', 'Great.', 'Well, bye.', 'Bye.', 'Bye.', 'Bye. ', 'Should we have invited her for lunch?', 'No.', \"We're going to start Season Two of Battlestar Galactica.\", 'We already watched the Season Two DVDs.', 'Not with commentary.', 'I think we should be good neighbors, invite her over, make her feel welcome.', 'We never invited Louis-slash-Louise over.', 'Well, then that was wrong of us.', 'We need to widen our circle.', 'I have a very wide circle.', 'I have 212 friends on myspace. ', \"Yes, and you've never met one of them.\", \"That's the beauty of it.\", \"I'm going to invite her over.\", \"We'll have a nice meal and chat.\", 'Chat?', \"We don't chat.\", 'At least not offline.', \"Well it's not difficult, you just listen to what she says and then you say something appropriate in response.\", 'To what end?', 'Hi.', 'Again.', 'Hi.', 'Hi.', 'Hi.', 'Hi. ', 'Anyway, um.', 'We brought home Indian food.', 'And, um.', \"I know that moving can be stressful, and I find that when I'm undergoing stress, that good food and company can have a comforting effect.\", \"Also, curry is a natural laxative, and I don't have to tell you that, uh, a clean colon is just one less thing to worry about.\", \"Leonard, I'm not expert here but I believe in the context of a luncheon invitation, you might want to skip the reference to bowel movements.\", \"Oh, you're inviting me over to eat?\", 'Uh, yes.', \"Oh, that's so nice, I'd love to.\", 'Great.', 'So, what do you guys do for fun around here?', 'Well, today we tried masturbating for money.', 'Okay, well, make yourself at home.', 'Okay, thankyou.', \"You're very welcome.\", 'This looks like some serious stuff, Leonard, did you do this?', \"Actually that's my work.\", 'Wow.', \"Yeah, well, it's just some quantum mechanics, with a little string theory doodling around the edges.\", \"That part there, that's just a joke, it's a spoof of the Bourne-Oppenheimer approximation.\", \"So you're like, one of those, beautiful mind genius guys.\", 'Yeah. ', 'This is really impressive.', 'I have a board.', 'If you like boards, this is my board.', 'Holy smokes.', \"If by holy smokes you mean a derivative restatement of the kind of stuff you can find scribbled on the wall of any men's room at MIT, sure.\", 'What?', 'Oh, come on.', \"Who hasn't seen this differential below 'here I sit broken hearted?'\", \"At least I didn't have to invent twenty-six dimensions just to make the math come out.\", \"I didn't invent them, they're there.\", 'In what universe?', 'In all of them, that is the point.', 'Uh, do you guys mind if I start?', \"Um, Penny, that's where I sit.\", 'So, sit next to me. ', 'No, I sit there.', \"What's the difference?\", \"What's the difference?\", 'Here we go.', 'In the winter that seat is close enough to the radiator to remain warm, and yet not so close as to cause perspiration.', \"In the summer it's directly in the path of a cross breeze created by open windows there, and there.\", \"It faces the television at an angle that is neither direct, thus discouraging conversation, nor so far wide to create a parallax distortion, I could go on, but I think I've made my point. \", 'Do you want me to move?', 'Well.', 'Just sit somewhere else.', 'Fine. ', 'Sheldon, sit!', 'Aaah!', 'Well, this is nice.', \"We don't have a lot of company over.\", \"That's not true.\", 'Koothrapali and Wolowitz come over all the time. ', 'Yes I know, but...', 'Tuesday night we played Klingon boggle until one in the morning.', 'Yes, I remember.', \"I resent you saying we don't have company.\", \"I'm sorry.\", 'That is an antisocial implication.', \"I said I'm sorry.\", 'So, Klingon boggle?', \"Yeah, it's like regular boggle but, in Klingon.\", \"That's probably enough about us, tell us about you.\", \"Um, me, okay, I'm Sagittarius, which probably tells you way more than you need to know.\", \"Yes, it tells us that you participate in the mass cultural delusion that the Sun's apparent position relative to arbitrarily defined constellations and the time of your birth somehow effects your personality.\", 'Participate in the what?', \"I think what Sheldon's trying to say, is that Sagittarius wouldn't have been our first guess.\", \"Oh, yeah, a lot of people think I'm a water sign.\", \"Okay, let's see, what else, oh, I'm a vegetarian, oh, except for fish, and the occasional steak, I love steak. \", \"That's interesting.\", \"Leonard can't process corn.\", 'Wu-uh, do you have some sort of a job?', \"Oh, yeah, I'm a waitress at the Cheesecake Factory.\", 'Oh, okay.', 'I love cheesecake.', \"You're lactose intolerant. \", \"I don't eat it, I just think it's a good idea.\", \"Oh, anyways, I'm also writing a screenplay.\", \"It's about this sensitive girl who comes to L.A from Lincoln Nebraska to be an actress, and winds up a waitress at the Cheesecake Factory.\", \"So it's based on your life?\", \"No, I'm from Omaha. \", 'Well, if that was a movie I would go see it.', 'I know, right?', \"Okay, let's see, what else?\", \"Um, that's about it.\", \"That's the story of Penny.\", 'Well it sounds wonderful.', 'It was.', 'Until I fell in love with a jerk. ', \"What's happening.\", \"I don't know.\", \"Oh God, you know, four years I lived with him, four years, that's like as long as High School. \", 'It took you four years to get through High School?', \"Don't.\", \"I just, I can't believe I trusted him.\", 'Should I say something?', 'I feel like I should say something.', 'You?', \"No, you'll only make it worse.\", 'You want to know the most pathetic part?', 'Even though I hate his lying, cheating guts, I still love him.', 'Is that crazy?', 'Yes. ', \"No, it's not crazy it's, uh, uh, it's a paradox.\", 'And paradoxes are part of nature, think about light.', 'Now if you look at Huygens, light is a wave, as confirmed by the double slit experiments, but then, along comes Albert Einstein and discovers that light behaves like particles too.', \"Well, I didn't make it worse.\", \"Oh, I'm so sorry, I'm such a mess, and on top of everything else I'm all gross from moving and my stupid shower doesn't even work.\", 'Our shower works.', 'Really?', 'Would it be totally weird if I used it?', 'Yes. ', 'No.', 'No?', 'No.', 'No.', \"It's right down the hall.\", 'Thanks.', 'You guys are really sweet.', 'Well, this is an interesting development. ', 'How so?', \"It has been some time since we've had a woman take her clothes off in our apartment.\", \"That's not true, remember at Thanksgiving my grandmother with Alzheimer's had that episode.\", 'Point taken.', \"It has been some time since we've had a woman take her clothes off after which we didn't want to rip our eyes out. \", 'The worst part was watching her carve that turkey.', 'So, what exactly are you trying to accomplish here?', 'Excuse me?', \"That woman in there's not going to have sex with you.\", \"Well I'm not trying to have sex with her.\", 'Oh, good.', \"Then you won't be disappointed.\", \"What makes you think she wouldn't have sex with me, I'm a male and she's a female?\", 'Yes, but not of the same species.', \"I'm not going to engage in hypotheticals here, I'm just trying to be a good neighbor.\", 'Oh, of course.', \"That's not to say that if a carnal relationship were to develop that I wouldn't participate.\", 'However briefly.', 'Do you think this possibility will be helped or hindered when she discovers your Luke Skywalker no-more-tears shampoo?', \"It's Darth Vader shampoo.Luke\", \"Skywalker's the conditioner.\", 'Wait till you see this.', \"It's fantastic.\", 'Unbelievable.', 'See what?', \"It's a Stephen Hawking lecture from MIT in 1974.\", 'This is not a good time.', \"It's before he became a creepy computer voice.\", \"That's great, you guys have to go.\", 'Why?', \"It's just not a good time.\", 'Leonard has a lady over.', 'Yeah, right, your grandmother back in town?', 'No.', \"And she's not a lady, she's just a new neighbor.\", 'Hang on, there really is a lady here?', 'Uh-huh.', \"And you want us out because you're anticipating coitus?\", \"I'm not anticipating coitus.\", \"So she's available for coitus?\", 'Can we please stop saying coitus?', 'Technically that would be coitus interruptus.', 'Hey, is there a trick to getting it to switch from tub to shower?', 'Oh.', 'Hi, sorry.', 'Hello!', 'Enchante Madamoiselle.', 'Howard Wolowitz, Cal-Tech department of Applied Physics.', \"You may be familiar with some of my work, it's currently orbiting Jupiter's largest moon taking high-resolution digital photographs.\", 'Penny.', 'I work at the Cheesecake Factory.', \"Come on, I'll show you the trick with the shower.\", 'Bon douche.', \"I'm sorry?\", \"It's French for good shower.\", \"It's a sentiment I can express in six languages.\", 'Save it for your blog, Howard.', 'See-ka-tong-guay-jow.', \"Uh, there it goes, it sticks, I'm sorry.\", 'Okay.', 'Thanks. ', \"You're welcome, oh, you're going to step right, okay, I'll...\", 'Hey, Leonard?', \"The hair products are Sheldon's.\", 'Um, okay.', 'Can I ask you a favour.', 'A favour?', 'Sure, you can ask me a favour, I would do you a favour for you.', \"It's okay if you say no.\", \"Oh, I'll probably say yes.\", \"It's just not the kind of thing you ask a guy you've just met.\", 'Wow.', 'I really think we should examine the chain of causality here.', 'Must we?', 'Event A.', 'A beautiful woman stands naked in our shower.', 'Event B.', \"We drive half way across town to retrieve a television set from the aforementioned woman's ex-boyfriend.\", 'Query, on what plane of existence is there even a semi-rational link between these events?', 'She asked me to do her a favour, Sheldon.', 'Ah, yes, well that may be the proximal cause of our journey, but we both know it only exists in contradistinction to the higher level distal cause.', 'Which is?', 'You think with your penis.', \"That's a biological impossibility and you didn't have to come.\", 'Oh, right, yes, I could have stayed behind and watched Wolowitz try to hit on Penny in Russian, Arabic and Farsi.', \"Why can't she get her own TV.\", 'Come on, you know how it is with break-ups.', \"No I don't.\", 'And neither do you.', 'Wuh, I, I broke up with Joyce Kim.', 'You did not break up with Joyce Kim, she defected to North Korea.', 'To mend her broken heart.', 'This situation is much less complicated.', \"There's some kind of dispute between Penny and her ex-boyfriend as to who gets custody of the TV.\", 'She just wanted to avoid having a scene with him.', 'So we get to have a scene with him?', \"No, Sheldon, there's not going to be a scene.\", \"There's two of us and one of him.\", \"Leonard, the two of us can't even carry a TV. \", 'So, you guys work with Leonard and Sheldon at the University?', \"Uh, I'm sorry, do you speak English?\", \"Oh, he speaks English, he just can't speak to women.\", 'Really, why?', \"He's kind of a nerd.\", 'Juice box?', \"I'll do the talking.\", 'Yeah.', \"Hi, I'm Leonard, this is Sheldon.\", 'Hello.', 'What did I just...', \"Uh, we're here to pick up Penny's TV.\", 'Get lost.', 'Okay, thanks for your time.', \"We're not going to give up just like that.\", \"Leonard, the TV is in the building, we've been denied access to the building, ergo we are done.\", 'Excuse me, if I were to give up at the first little hitch I never would have been able to identify the fingerprints of string theory in the aftermath of the big bang.', 'My apologies.', \"What's your plan.\", \"It's just a privilege to watch your mind at work.\", 'Come on, we have a combined IQ of 360, we should be able to figure out how to get into a stupid building.', 'What do you think their combined IQ is?', 'Just grab the door.', 'This is it.', \"I'll do the talking.\", \"Good thinking, I'll just be the muscle.\", 'Yeah?', \"I'm Leonard, this is Sheldon.\", 'From the intercom.', 'How the hell did you get in the building?', 'Oh.', \"We're scientists.\", 'Tell him about our IQ.', 'Leonard.', 'What?', 'My mom bought me those pants.', \"I'm sorry.\", \"You're going to have to call her.\", \"Sheldon, I'm so sorry I dragged you through this.\", \"It's okay.\", \"It wasn't my first pantsing, and it won't be my last.\", 'And you were right about my motives, I was hoping to establish a relationship with Penny that might have some day led to sex.', 'Well you got me out of my pants.', \"Anyway, I've learned my lesson.\", \"She's out of my league, I'm done with her, I've got my work, one day I'll win the Nobel Prize and then I'll die alone.\", \"Don't think like that, you're not going to die alone.\", \"Thank you Sheldon, you're a good friend.\", \"And you're certainly not going to win a Nobel Prize.\", 'This is one of my favourite places to kick back after a quest, they have a great house ale.', 'Wow, cool tiger.', \"Yeah, I've had him since level ten.\", 'His name is Buttons.', 'Anyway, if you had your own game character we could hang out, maybe go on a quest.', 'Uh, sounds interesting.', \"So you'll think about it?\", \"Oh, I don't think I'll be able to stop thinking about it.\", 'Smooth.', \"We're home.\", 'Oh, my God, what happened?', 'Well, your ex-boyfriend sends his regards and I think the rest is fairly self-explanatory.', \"I'm so sorry, I really thought if you guys went instead of me he wouldn't be such an ass.\", 'No, it was a valid hypothesis.', 'That was a valid hypothesis?', 'What is happening to you?', \"Really, thank you so much for going and trying you're, uh, you're so terrific.\", \"Why don't you put some clothes on, I'll get my purse and dinner is on me, okay?\", 'Really?', 'Great.', 'Thank you.', \"You're not done with her, are you?\", 'Our babies will be smart and beautiful.', 'Not to mention imaginary.', 'Is Thai food okay with you Penny?', 'Sure.', \"We can't have Thai food, we had Indian for lunch.\", 'So?', \"They're both curry based cuisines.\", 'So?', 'They would be gastronomically redundant.', \"I can see we're going to have to spell out everything for this girl.\", 'Any ideas Raj? ', 'Turn left on Lake Street and head up to Colorado.', 'I know a wonderful little sushi bar that has karaoke.', 'That sounds like fun.', \"Baby, baby don't get hooked on me.\", \"Uh, baby, baby don't get hooked on me.\", \"I don't know what your odds are in the world as a whole, but as far as the population of this car goes, you're a veritable Mack Daddy.\"] ['sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'receptionist', 'leonard', 'leonard', 'leonard', 'receptionist', 'leonard', 'leonard', 'receptionist', 'sheldon', 'receptionist', 'leonard', 'leonard', 'receptionist', 'receptionist', 'receptionist', 'sheldon', 'leonard', 'leonard', 'sheldon', 'sheldon', 'sheldon', 'sheldon', 'leonard', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'receptionist', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'leonard', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'penny', 'leonard', 'sheldon', 'leonard', 'sheldon', 'penny', 'leonard', 'penny', 'leonard', 'penny', 'leonard', 'penny', 'leonard', 'sheldon', 'penny', 'leonard', 'leonard', 'penny', 'leonard', 'penny', 'sheldon', 'leonard', 'leonard', 'penny', 'sheldon', 'leonard', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'leonard', 'sheldon', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'leonard', 'penny', 'sheldon', 'leonard', 'penny', 'leonard', 'leonard', 'leonard', 'leonard', 'leonard', 'sheldon', 'penny', 'leonard', 'penny', 'leonard', 'penny', 'sheldon', 'leonard', 'penny', 'leonard', 'penny', 'sheldon', 'penny', 'sheldon', 'sheldon', 'penny', 'sheldon', 'penny', 'leonard', 'leonard', 'penny', 'sheldon', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'penny', 'sheldon', 'penny', 'sheldon', 'penny', 'sheldon', 'leonard', 'sheldon', 'sheldon', 'sheldon', 'penny', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'penny', 'leonard', 'leonard', 'penny', 'sheldon', 'penny', 'leonard', 'penny', 'penny', 'sheldon', 'sheldon', 'leonard', 'penny', 'leonard', 'leonard', 'sheldon', 'leonard', 'penny', 'penny', 'leonard', 'penny', 'leonard', 'penny', 'penny', 'penny', 'penny', 'leonard', 'penny', 'penny', 'sheldon', 'leonard', 'penny', 'sheldon', 'leonard', 'penny', 'leonard', 'leonard', 'sheldon', 'sheldon', 'penny', 'penny', 'penny', 'sheldon', 'leonard', 'leonard', 'leonard', 'leonard', 'penny', 'leonard', 'penny', 'penny', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'penny', 'penny', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'leonard', 'sheldon', 'leonard', 'leonard', 'howard', 'raj', 'raj', 'leonard', 'howard', 'leonard', 'howard', 'leonard', 'raj', 'leonard', 'sheldon', 'howard', 'leonard', 'leonard', 'howard', 'leonard', 'howard', 'leonard', 'howard', 'leonard', 'sheldon', 'penny', 'penny', 'penny', 'penny', 'howard', 'howard', 'howard', 'penny', 'penny', 'leonard', 'howard', 'penny', 'howard', 'howard', 'leonard', 'howard', 'leonard', 'penny', 'penny', 'leonard', 'penny', 'leonard', 'penny', 'penny', 'leonard', 'leonard', 'penny', 'leonard', 'penny', 'leonard', 'sheldon', 'leonard', 'sheldon', 'sheldon', 'sheldon', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'leonard', 'leonard', 'leonard', 'sheldon', 'leonard', 'leonard', 'sheldon', 'penny', 'penny', 'howard', 'penny', 'howard', 'howard', 'leonard', 'voice_from_buzzer', 'leonard', 'sheldon', 'leonard', 'leonard', 'voice', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'leonard', 'leonard', 'sheldon', 'enormous_man', 'leonard', 'sheldon', 'man', 'leonard', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'leonard', 'sheldon', 'leonard', 'sheldon', 'howard', 'penny', 'howard', 'howard', 'howard', 'penny', 'howard', 'penny', 'raj', 'leonard', 'penny', 'leonard', 'penny', 'leonard', 'sheldon', 'sheldon', 'penny', 'penny', 'leonard', 'leonard', 'sheldon', 'sheldon', 'leonard', 'sheldon', 'leonard', 'penny', 'sheldon', 'penny', 'sheldon', 'penny', 'sheldon', 'sheldon', 'penny', 'howard', 'howard', 'penny', 'howard', 'howard', 'sheldon']\n"
     ]
    }
   ],
   "source": [
    "for x,y in zip(X,Y):\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n",
      "[[6597, 3994, 6329, 2163, 7702, 5246, 8520, 6329, 149, 3939, 818, 4668, 6880, 1140, 8191, 4768, 4629, 7702, 5182, 1140, 3685, 8087, 837, 8520, 1533, 4668, 8367], [3994, 1140, 7151, 1140, 3685, 3336, 7496, 3336, 3994, 1140, 5182, 6006, 1140, 6507, 934, 149, 773, 8034, 1140, 1426, 3684, 3737, 3336, 1140, 3685, 8087, 6252, 702, 8520, 1533, 4668, 8367]]\n",
      "tensor([  27,   32,    6,  ...,    8,    2,    2])\n",
      "tensor([[  934,  2177,  2526,  ...,  1903,  8367,  8367],\n",
      "        [ 6089,  2812,  3336,  ...,     0,     0,     0],\n",
      "        [ 5781,  5567,  2177,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 5853,     0,     0,  ...,     0,     0,     0],\n",
      "        [ 2167,     0,     0,  ...,     0,     0,     0],\n",
      "        [ 6124,     0,     0,  ...,     0,     0,     0]])\n",
      "tensor([[[-6.5334e-02, -9.3031e-02, -1.7571e-02,  ...,  1.6642e-01,\n",
      "          -1.3079e-01,  3.5397e-02],\n",
      "         [-4.0083e-01,  1.3992e-01,  1.2907e-01,  ...,  2.3206e-01,\n",
      "           7.9414e-02,  1.3813e-01],\n",
      "         [-3.6183e-03,  3.7913e-01, -8.1589e-02,  ...,  2.4142e-01,\n",
      "           2.3247e-01,  2.7664e-01],\n",
      "         ...,\n",
      "         [-1.8483e-01, -8.5436e-02, -1.2113e-01,  ...,  9.7873e-02,\n",
      "           4.7634e-01, -6.5887e-02],\n",
      "         [-1.1112e-01, -1.3859e-03, -1.7780e-01,  ...,  6.3374e-02,\n",
      "          -1.2161e-01,  3.9339e-02],\n",
      "         [-1.1112e-01, -1.3859e-03, -1.7780e-01,  ...,  6.3374e-02,\n",
      "          -1.2161e-01,  3.9339e-02]],\n",
      "\n",
      "        [[-1.1586e-02, -7.6437e-02, -2.4990e-01,  ...,  1.5718e-01,\n",
      "           2.1977e-01,  1.2105e-01],\n",
      "         [-5.3672e-02,  1.1807e-01, -2.9748e-02,  ...,  1.5418e-01,\n",
      "           4.2013e-01, -6.6365e-02],\n",
      "         [-2.3167e-02, -4.2483e-03, -1.0572e-01,  ...,  8.9398e-02,\n",
      "          -1.5900e-02,  1.4866e-01],\n",
      "         ...,\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01],\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01],\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01]],\n",
      "\n",
      "        [[ 1.9851e-02, -1.4669e-01, -2.0452e-01,  ...,  1.1820e-02,\n",
      "           1.8933e-01, -1.6416e-01],\n",
      "         [ 3.0817e-02,  1.5637e-01, -9.8392e-02,  ...,  8.7291e-02,\n",
      "           1.8826e-01, -1.1722e-01],\n",
      "         [-4.0083e-01,  1.3992e-01,  1.2907e-01,  ...,  2.3206e-01,\n",
      "           7.9414e-02,  1.3813e-01],\n",
      "         ...,\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01],\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01],\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.4926e-01,  2.1572e-01,  1.8823e-01,  ...,  3.2328e-01,\n",
      "           3.8224e-01, -8.6828e-02],\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01],\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01],\n",
      "         ...,\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01],\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01],\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01]],\n",
      "\n",
      "        [[-1.4189e-01, -4.9650e-02,  5.2389e-02,  ...,  2.5631e-01,\n",
      "           3.9387e-01, -1.3451e-01],\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01],\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01],\n",
      "         ...,\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01],\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01],\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01]],\n",
      "\n",
      "        [[-1.7519e-01, -7.2514e-02, -3.6520e-01,  ...,  8.3689e-02,\n",
      "           1.0878e-01, -8.0402e-02],\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01],\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01],\n",
      "         ...,\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01],\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01],\n",
      "         [-2.1633e-01,  1.9520e-01, -3.9172e-01,  ...,  2.0473e-01,\n",
      "           1.3574e-01, -1.3746e-01]]])\n",
      "PackedSequence(data=tensor([[-6.5334e-02, -9.3031e-02, -1.7571e-02,  ...,  1.6642e-01,\n",
      "         -1.3079e-01,  3.5397e-02],\n",
      "        [-4.0083e-01,  1.3992e-01,  1.2907e-01,  ...,  2.3206e-01,\n",
      "          7.9414e-02,  1.3813e-01],\n",
      "        [-3.6183e-03,  3.7913e-01, -8.1589e-02,  ...,  2.4142e-01,\n",
      "          2.3247e-01,  2.7664e-01],\n",
      "        ...,\n",
      "        [-1.4926e-01,  2.1572e-01,  1.8823e-01,  ...,  3.2328e-01,\n",
      "          3.8224e-01, -8.6828e-02],\n",
      "        [-1.4189e-01, -4.9650e-02,  5.2389e-02,  ...,  2.5631e-01,\n",
      "          3.9387e-01, -1.3451e-01],\n",
      "        [-1.7519e-01, -7.2514e-02, -3.6520e-01,  ...,  8.3689e-02,\n",
      "          1.0878e-01, -8.0402e-02]]), batch_sizes=tensor([ 12702,  12679,  11295,  10277,   9153,   8055,   7039,   6192,\n",
      "          5384,   4652,   4042,   3530,   3114,   2722,   2401,   2089,\n",
      "          1836,   1627,   1445,   1281,   1122,    985,    866,    762,\n",
      "           680,    591,    527,    477,    419,    380,    334,    293,\n",
      "           257,    228,    201,    178,    154,    136,    124,    113,\n",
      "           100,     87,     76,     65,     60,     55,     49,     44,\n",
      "            37,     34,     30,     28,     25,     22,     19,     18,\n",
      "            17,     17,     16,     14,     13,     11,     11,     10,\n",
      "             9,      8,      8,      8,      7,      7,      7,      7,\n",
      "             7,      7,      6,      5,      5,      5,      5,      5,\n",
      "             5,      4,      4,      4,      3,      2,      2,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1]))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lstm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-683c497a2e53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# throw them through your LSTM (remember to give batch_first=True here if you packed with it)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mpacked_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mht\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lstm' is not defined"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "embed = nn.Embedding(len(words_set), len(get_word_vector(X[0][0])))\n",
    "embed.weight.data.copy_(we.vectors[we_idx])\n",
    "embed.weight.requires_grad = False\n",
    "\n",
    "idx_set_words = dict(zip(list(words_set), range(len(words_set))))\n",
    "\n",
    "vectorized_seqs = [[idx_set_words[w] for w in s]for s in X]\n",
    "print(vectorized_seqs[0:2])\n",
    "\n",
    "# get the length of each seq in your batch\n",
    "seq_lengths = torch.LongTensor(list(map(len, vectorized_seqs)))\n",
    "print(seq_lengths)\n",
    "# dump padding everywhere, and place seqs on the left.\n",
    "# NOTE: you only need a tensor as big as your longest sequence\n",
    "seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
    "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "    seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "\n",
    "# SORT YOUR TENSORS BY LENGTH!\n",
    "seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "seq_tensor = seq_tensor[perm_idx]\n",
    "\n",
    "# utils.rnn lets you give (B,L,D) tensors where B is the batch size, L is the maxlength, if you use batch_first=True\n",
    "# Otherwise, give (L,B,D) tensors\n",
    "seq_tensor = seq_tensor.transpose(0,1) # (B,L,D) -> (L,B,D)\n",
    "print(seq_tensor)\n",
    "\n",
    "# embed your sequences\n",
    "seq_tensor = embed(seq_tensor)\n",
    "print(seq_tensor)\n",
    "\n",
    "# pack them up nicely\n",
    "packed_input = pack_padded_sequence(seq_tensor, seq_lengths.cpu().numpy())\n",
    "print(packed_input)\n",
    "\n",
    "# throw them through your LSTM (remember to give batch_first=True here if you packed with it)\n",
    "packed_output, (ht, ct) = lstm(packed_input)\n",
    "print(packed_output)\n",
    "\n",
    "# unpack your output if required\n",
    "output, _ = pad_packed_sequence(packed_output)\n",
    "print(output)\n",
    "\n",
    "# Or if you just want the final hidden state?\n",
    "print(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '-', 'a', 'c', 'd', 'e', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 's', 't']\n",
      "[10, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "def flatten(l):\n",
    "    return list(itertools.chain.from_iterable(l))\n",
    "\n",
    "seqs = ['ghatmasala','nicela','c-pakodas']\n",
    "\n",
    "# make <pad> idx 0\n",
    "vocab = ['<pad>'] + sorted(list(set(flatten(seqs))))\n",
    "print(vocab)\n",
    "\n",
    "vectorized_seqs = [[vocab.index(tok) for tok in seq]for seq in seqs]\n",
    "\n",
    "# get the length of each seq in your batch\n",
    "seq_lengths = map(len, vectorized_seqs)\n",
    "print(list(seq_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        seq_len = len(word_inputs)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://www.kdnuggets.com/2018/06/taming-lstms-variable-sized-mini-batches-pytorch.html\n",
    "#https://github.com/EdGENetworks/attention-networks-for-classification/blob/master/model.py\n",
    "#https://github.com/EdGENetworks/attention-networks-for-classification/blob/master/attention_model_validation_experiments.ipynb\n",
    "#https://explosion.ai/blog/deep-learning-formula-nlp\n",
    "#https://github.com/koustuvsinha/hred-py/blob/master/hred_pytorch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'HierarchicalRNN/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a55e747666fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HierarchicalRNN/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'HierarchicalRNN/'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('HierarchicalRNN/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data as dt\n",
    "import model as md\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "import numpy as np\n",
    "import itertools\n",
    "import csv\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torchtext.vocab as vocab\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def save(nameFile, toSave):\n",
    "    pickle_out = open(nameFile+\".pickle\", \"wb\")\n",
    "    pickle.dump(toSave, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "def load(nameFile):\n",
    "    pickle_in = open(nameFile+\".pickle\", \"rb\")\n",
    "    return pickle.load(pickle_in)\n",
    "\n",
    "def get_word_vector(word):\n",
    "    return we.vectors[we.stoi[word]]\n",
    "\n",
    "def closest(vec, n=2):#10):\n",
    "    \"\"\"\n",
    "    Find the closest words for a given vector\n",
    "    \"\"\"\n",
    "    all_dists = [(w, torch.dist(vec, get_word_vector(w))) for w in we.itos]\n",
    "    return sorted(all_dists, key=lambda t: t[1])[:n]\n",
    "\n",
    "def print_tuples(tuples):\n",
    "    for tuple in tuples:\n",
    "        print('(%.4f) %s' % (tuple[1], tuple[0]))\n",
    "\n",
    "# In the form w1 : w2 :: w3 : ?\n",
    "def analogy(w1, w2, w3, n=5, filter_given=True):\n",
    "    print('\\n[%s : %s :: %s : ?]' % (w1, w2, w3))\n",
    "   \n",
    "    # w2 - w1 + w3 = w4\n",
    "    closest_words = closest(get_word_vector(w2) - get_word_vector(w1) + get_word_vector(w3))\n",
    "    \n",
    "    # Optionally filter out given words\n",
    "    if filter_given:\n",
    "        closest_words = [t for t in closest_words if t[0] not in [w1, w2, w3]]\n",
    "\n",
    "    print_tuples(closest_words[:n])\n",
    "\n",
    "def load_data(path_transcripts='/vol/work2/galmant/transcripts/', type_sentence_embedding='lstm'):\n",
    "    punctuations_end_sentence = ['.', '?', '!']\n",
    "\n",
    "    we = None\n",
    "    if type_sentence_embedding == 'lstm':\n",
    "        we = vocab.FastText(language='en')\n",
    "        '''\n",
    "        pretrained_aliases = {\n",
    "            \"charngram.100d\": partial(CharNGram),\n",
    "            \"fasttext.en.300d\": partial(FastText, language=\"en\"),\n",
    "            \"fasttext.simple.300d\": partial(FastText, language=\"simple\"),\n",
    "            \"glove.42B.300d\": partial(GloVe, name=\"42B\", dim=\"300\"),\n",
    "            \"glove.840B.300d\": partial(GloVe, name=\"840B\", dim=\"300\"),\n",
    "            \"glove.twitter.27B.25d\": partial(GloVe, name=\"twitter.27B\", dim=\"25\"),\n",
    "            \"glove.twitter.27B.50d\": partial(GloVe, name=\"twitter.27B\", dim=\"50\"),\n",
    "            \"glove.twitter.27B.100d\": partial(GloVe, name=\"twitter.27B\", dim=\"100\"),\n",
    "            \"glove.twitter.27B.200d\": partial(GloVe, name=\"twitter.27B\", dim=\"200\"),\n",
    "            \"glove.6B.50d\": partial(GloVe, name=\"6B\", dim=\"50\"),\n",
    "            \"glove.6B.100d\": partial(GloVe, name=\"6B\", dim=\"100\"),\n",
    "            \"glove.6B.200d\": partial(GloVe, name=\"6B\", dim=\"200\"),\n",
    "            \"glove.6B.300d\": partial(GloVe, name=\"6B\", dim=\"300\")\n",
    "        }\n",
    "        '''\n",
    "\n",
    "    X_all = []\n",
    "    Y_all = []\n",
    "    words_set = set()\n",
    "    for f in sorted(glob.glob(path_transcripts+'*')):\n",
    "        with open(f, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "            X_ = []\n",
    "            Y_ = []\n",
    "            for row in reader:\n",
    "                sentence = row[2]\n",
    "                old_word = row[2]\n",
    "                for word in row[3:]:\n",
    "                    if any(punctuation in old_word for punctuation in punctuations_end_sentence) and word and word[0].isupper():\n",
    "                        X_.append(sentence)\n",
    "                        Y_.append(row[1])\n",
    "                        sentence = word\n",
    "                    else:\n",
    "                        sentence += ' '+word\n",
    "                    old_word = word\n",
    "                if sentence and row[1]:\n",
    "                    X_.append(sentence)\n",
    "                    Y_.append(row[1])\n",
    "            Y = [s.lower() for s in Y_]\n",
    "            if type_sentence_embedding == 'lstm':\n",
    "                X = [s.lower().split() for s in X_]\n",
    "                #Y = [s.lower() for s in Y_]\n",
    "                to_del = []\n",
    "                for s in X:\n",
    "                    for w in s:\n",
    "                        if w not in we.stoi:\n",
    "                            to_del.append(w)\n",
    "                X = [[w for w in s if w not in to_del] for s in X]\n",
    "                for words_per_sentence in X:\n",
    "                    words_set = words_set.union(set(words_per_sentence))\n",
    "            else:\n",
    "                X = X_\n",
    "                Y = Y_\n",
    "            if len(X)>0 and len(Y)>0:\n",
    "                X_all.append(X)\n",
    "                Y_all.append(Y)\n",
    "            assert len(X) == len(Y)\n",
    "\n",
    "    threshold_train_dev = int(len(X_all)*0.8)\n",
    "    threshold_dev_test = threshold_train_dev + int(len(X_all)*0.1)\n",
    "    X_train = X_all[:threshold_train_dev]\n",
    "    Y_train = Y_all[:threshold_train_dev]\n",
    "    X_dev = X_all[threshold_train_dev:threshold_dev_test]\n",
    "    Y_dev = Y_all[threshold_train_dev:threshold_dev_test]\n",
    "    X_test = X_all[threshold_dev_test:]\n",
    "    Y_test = Y_all[threshold_dev_test:]\n",
    "    return X_train, Y_train, X_dev, Y_dev, X_test, Y_test, words_set, we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "is_trained = False\n",
    "type_sentence_embedding='infersent'\n",
    "\n",
    "X_train, Y_train, X_dev, Y_dev, X_test, Y_test, words_set, we = load_data(type_sentence_embedding=type_sentence_embedding)\n",
    "hidden_size = 300\n",
    "batch_size=32\n",
    "if type_sentence_embedding == 'lstm':\n",
    "    taille_embedding = len(we.vectors[we.stoi[X_train[0][0][0]]])\n",
    "else:\n",
    "    taille_embedding = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import torchtext.vocab as vocab\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import math\n",
    "import itertools\n",
    "from random import shuffle\n",
    "from models import InferSent\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "V = 2\n",
    "MODEL_PATH = '/vol/work3/maurice/encoder/infersent%s.pickle' % V\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "infersent = InferSent(params_model)\n",
    "infersent.load_state_dict(torch.load(MODEL_PATH))\n",
    "W2V_PATH = '/vol/work3/maurice/dataset/fastText/crawl-300d-2M-subword.vec'\n",
    "infersent.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "def split_by_context(X, Y, taille_context, batch_size, device='cpu'): #(8..,1,4096)\n",
    "    list_tensors_X = []\n",
    "    list_tensors_Y = []\n",
    "    for i in range(X.shape[0] - (2*taille_context + 1)): #NOT 1 NOW 0 FOR THE NUMBER OF SENTENCES\n",
    "        list_tensors_X.append(torch.index_select(X, 0, torch.tensor(list(range(i,i+2*(taille_context+1))), device=device))) #(8,1,4096)\n",
    "        list_tensors_Y.append(torch.index_select(Y, 0, torch.tensor([taille_context], device=device))) #(1)\n",
    "    tensor_split_X = torch.stack(list_tensors_X).transpose(0,1).view(2*(taille_context+1),-1,X.shape[-1]) #(n-8,8,1,4096) -> (8,n,1,4096) -> (8,n,4096)\n",
    "    tensor_split_Y = torch.stack(list_tensors_Y).view(1,-1)#.squeeze(0) #.transpose(0,1) #(n,1) -> (1,n)\n",
    "    minis_batch_X = {}\n",
    "    minis_batch_Y = {}\n",
    "    nb_batches = int(tensor_split_X.shape[1]/batch_size)\n",
    "    for i in range(nb_batches):\n",
    "        minis_batch_X[i] = tensor_split_X[:,i*batch_size:(i+1)*batch_size,:]\n",
    "        minis_batch_Y[i] = tensor_split_Y[:,i*batch_size:(i+1)*batch_size]\n",
    "    minis_batch_X[nb_batches] = tensor_split_X[:,nb_batches*batch_size:,:] #n/32 tensors of (8,32,4096)\n",
    "    minis_batch_Y[nb_batches] = tensor_split_Y[:,nb_batches*batch_size:] #n/32 tensors of (1,32)\n",
    "    shuffle_ids = sample(list(range(nb_batches+1)), k=nb_batches+1)\n",
    "    return [minis_batch_X[i] for i in shuffle_ids], [minis_batch_Y[i] for i in shuffle_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 0 on 176\n",
      "Found 927(/934) words with w2v vectors\n",
      "Vocab size : 927\n",
      "file 1 on 176\n",
      "Found 952(/957) words with w2v vectors\n",
      "Vocab size : 952\n",
      "mean poucentages majority class 0.7201168309602044\n"
     ]
    }
   ],
   "source": [
    "X_all=X_train\n",
    "Y_all=Y_train\n",
    "taille_context=3\n",
    "bidirectional=False\n",
    "num_layers=1\n",
    "nb_epoch=5\n",
    "targset_size=1\n",
    "idx_set_words = None\n",
    "embed = None\n",
    "dim = 0\n",
    "\n",
    "model = md.HierarchicalBiLSTM_on_sentence_embedding(taille_embedding, hidden_size, targset_size, num_layers, bidirectional=bidirectional, device=device, type_sentence_embedding=type_sentence_embedding)\n",
    "model = model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=None)#pos_weight)#BCELoss()#NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, nesterov=True)\n",
    "#scheduler = StepLR(optimizer, step_size=math.ceil(nb_epoch/5), gamma=0.2)\n",
    "\n",
    "# Concatenate all the datas in pytorch lists of tensors and create the mini-batch (8,32,4096) or (109,8*32,300)\n",
    "season_episode = 0\n",
    "poucentages_majority_class = []\n",
    "inputs_embeddings = []\n",
    "outputs_refs = []\n",
    "for X_,Y_ in zip(X_all,Y_all):\n",
    "    if season_episode == 2:\n",
    "        break\n",
    "    print('file',season_episode,'on',len(X_all))\n",
    "    infersent.build_vocab(X_, tokenize=True)\n",
    "    words_embeddings = infersent.encode(X_, tokenize=True) #In fact it's sentences embeddings, just to have the same name !!! (B,D)\n",
    "    words_embeddings = torch.from_numpy(words_embeddings).unsqueeze(1)# (B,D) -> (L,B,D)\n",
    "    words_embeddings = words_embeddings.to(device)\n",
    "    #words_embeddings : (8..,1,4096) or (109,8..,300); 8.. -> number of sentences, 8 -> context size, 109 -> max number of words per sentence, 300 or 4096 -> embeddings size\n",
    "    Y, poucentage_majority_class = md.create_Y(Y_, device) #(8..)\n",
    "    #TODO CURRENTLY ONLY FOR INFERSENT\n",
    "    inputs_embeddings_, outputs_refs_ = split_by_context(words_embeddings, Y, taille_context, batch_size, device=device) #(n/32,8,32,4096) and (n/32,1,32)\n",
    "    inputs_embeddings += inputs_embeddings_\n",
    "    outputs_refs += outputs_refs_\n",
    "    poucentages_majority_class.append(poucentage_majority_class)\n",
    "    season_episode += 1\n",
    "print('mean poucentages majority class', sum(poucentages_majority_class)/len(poucentages_majority_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "bool value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2d96bb47454a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeddings\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: bool value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('inputs_embeddings.pickle', 'wb') as handle:\n",
    "    pickle.dump(inputs_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('outputs_refs.pickle', 'wb') as handle:\n",
    "    pickle.dump(outputs_refs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('inputs_embeddings.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "device = 'cuda:0'\n",
    "is_trained = False\n",
    "pre_load_data = False\n",
    "type_sentence_embedding='infersent'\n",
    "subset_data = 'big_bang_theory_:'\n",
    "path_save = '/vol/work3/maurice/HierarchicalRNN/'\n",
    "\n",
    "'''\n",
    "Data will be store like this :\n",
    "\n",
    "path_save/type_sentence_embedding/subset_data/models/pytorch_model_epoch_0.pth.tar\n",
    "path_save/type_sentence_embedding/subset_data/pytorch_best_model_epoch_0.pth.tar\n",
    "path_save/type_sentence_embedding/subset_data/pre_trained_features/train/inputs_embeddings_0.pickle\n",
    "path_save/type_sentence_embedding/subset_data/pre_trained_features/train/outputs_refs_0.pickle\n",
    "path_save/type_sentence_embedding/subset_data/pre_trained_features/dev/inputs_embeddings_0.pickle\n",
    "path_save/type_sentence_embedding/subset_data/pre_trained_features/dev/outputs_refs_0.pickle\n",
    "path_save/type_sentence_embedding/subset_data/pre_trained_features/test/inputs_embeddings_0.pickle\n",
    "path_save/type_sentence_embedding/subset_data/pre_trained_features/test/outputs_refs_0.pickle\n",
    "\n",
    "Where type_sentence_embedding is one of : lstm, infersent, ...\n",
    "and subset_data if one of : big_bang_theory_:, big_bang_theory_season_1:3, big_bang_theory_:_game_of_thrones_::\n",
    "'''\n",
    "\n",
    "pathlib.Path(path_save+type_sentence_embedding+'/'+subset_data+'/models/').mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(path_save+type_sentence_embedding+'/'+subset_data+'pre_trained_features/train/').mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(path_save+type_sentence_embedding+'/'+subset_data+'pre_trained_features/dev/').mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(path_save+type_sentence_embedding+'/'+subset_data+'pre_trained_features/test/').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /vol/work3/maurice/HierarchicalRNN/infersent/big_bang_theory_\\:/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 25, 4096])\n"
     ]
    }
   ],
   "source": [
    "sentences_emb=inputs_embeddings[0]\n",
    "input_previous_sentences = sentences_emb[:int(sentences_emb.shape[0]/2),:,:]#(B,L,D) -> (4,32,4096) -> (L,B,D)\n",
    "input_future_sentences = sentences_emb[int(sentences_emb.shape[0]/2):,:,:]#(B,L,D) -> (4,32,4096) -> (L,B,D)\n",
    "print(input_previous_sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "début train\n",
      "epoch 0 on 5\n",
      "0 on 24 epoch 0 on 5\n",
      "1 on 24 epoch 0 on 5\n",
      "2 on 24 epoch 0 on 5\n",
      "3 on 24 epoch 0 on 5\n",
      "4 on 24 epoch 0 on 5\n",
      "5 on 24 epoch 0 on 5\n",
      "6 on 24 epoch 0 on 5\n",
      "7 on 24 epoch 0 on 5\n",
      "8 on 24 epoch 0 on 5\n",
      "9 on 24 epoch 0 on 5\n",
      "10 on 24 epoch 0 on 5\n",
      "11 on 24 epoch 0 on 5\n",
      "12 on 24 epoch 0 on 5\n",
      "13 on 24 epoch 0 on 5\n",
      "14 on 24 epoch 0 on 5\n",
      "15 on 24 epoch 0 on 5\n",
      "16 on 24 epoch 0 on 5\n",
      "17 on 24 epoch 0 on 5\n",
      "18 on 24 epoch 0 on 5\n",
      "19 on 24 epoch 0 on 5\n",
      "20 on 24 epoch 0 on 5\n",
      "21 on 24 epoch 0 on 5\n",
      "22 on 24 epoch 0 on 5\n",
      "23 on 24 epoch 0 on 5\n",
      "0.018962384065768372\n",
      "epoch 1 on 5\n",
      "0 on 24 epoch 1 on 5\n",
      "1 on 24 epoch 1 on 5\n",
      "2 on 24 epoch 1 on 5\n",
      "3 on 24 epoch 1 on 5\n",
      "4 on 24 epoch 1 on 5\n",
      "5 on 24 epoch 1 on 5\n",
      "6 on 24 epoch 1 on 5\n",
      "7 on 24 epoch 1 on 5\n",
      "8 on 24 epoch 1 on 5\n",
      "9 on 24 epoch 1 on 5\n",
      "10 on 24 epoch 1 on 5\n",
      "11 on 24 epoch 1 on 5\n",
      "12 on 24 epoch 1 on 5\n",
      "13 on 24 epoch 1 on 5\n",
      "14 on 24 epoch 1 on 5\n",
      "15 on 24 epoch 1 on 5\n",
      "16 on 24 epoch 1 on 5\n",
      "17 on 24 epoch 1 on 5\n",
      "18 on 24 epoch 1 on 5\n",
      "19 on 24 epoch 1 on 5\n",
      "20 on 24 epoch 1 on 5\n",
      "21 on 24 epoch 1 on 5\n",
      "22 on 24 epoch 1 on 5\n",
      "23 on 24 epoch 1 on 5\n",
      "0.005045734210095058\n",
      "epoch 2 on 5\n",
      "0 on 24 epoch 2 on 5\n",
      "1 on 24 epoch 2 on 5\n",
      "2 on 24 epoch 2 on 5\n",
      "3 on 24 epoch 2 on 5\n",
      "4 on 24 epoch 2 on 5\n",
      "5 on 24 epoch 2 on 5\n",
      "6 on 24 epoch 2 on 5\n",
      "7 on 24 epoch 2 on 5\n",
      "8 on 24 epoch 2 on 5\n",
      "9 on 24 epoch 2 on 5\n",
      "10 on 24 epoch 2 on 5\n",
      "11 on 24 epoch 2 on 5\n",
      "12 on 24 epoch 2 on 5\n",
      "13 on 24 epoch 2 on 5\n",
      "14 on 24 epoch 2 on 5\n",
      "15 on 24 epoch 2 on 5\n",
      "16 on 24 epoch 2 on 5\n",
      "17 on 24 epoch 2 on 5\n",
      "18 on 24 epoch 2 on 5\n",
      "19 on 24 epoch 2 on 5\n",
      "20 on 24 epoch 2 on 5\n",
      "21 on 24 epoch 2 on 5\n",
      "22 on 24 epoch 2 on 5\n",
      "23 on 24 epoch 2 on 5\n",
      "0.0037903642708746097\n",
      "epoch 3 on 5\n",
      "0 on 24 epoch 3 on 5\n",
      "1 on 24 epoch 3 on 5\n",
      "2 on 24 epoch 3 on 5\n",
      "3 on 24 epoch 3 on 5\n",
      "4 on 24 epoch 3 on 5\n",
      "5 on 24 epoch 3 on 5\n",
      "6 on 24 epoch 3 on 5\n",
      "7 on 24 epoch 3 on 5\n",
      "8 on 24 epoch 3 on 5\n",
      "9 on 24 epoch 3 on 5\n",
      "10 on 24 epoch 3 on 5\n",
      "11 on 24 epoch 3 on 5\n",
      "12 on 24 epoch 3 on 5\n",
      "13 on 24 epoch 3 on 5\n",
      "14 on 24 epoch 3 on 5\n",
      "15 on 24 epoch 3 on 5\n",
      "16 on 24 epoch 3 on 5\n",
      "17 on 24 epoch 3 on 5\n",
      "18 on 24 epoch 3 on 5\n",
      "19 on 24 epoch 3 on 5\n",
      "20 on 24 epoch 3 on 5\n",
      "21 on 24 epoch 3 on 5\n",
      "22 on 24 epoch 3 on 5\n",
      "23 on 24 epoch 3 on 5\n",
      "0.0031950331661694995\n",
      "epoch 4 on 5\n",
      "0 on 24 epoch 4 on 5\n",
      "1 on 24 epoch 4 on 5\n",
      "2 on 24 epoch 4 on 5\n",
      "3 on 24 epoch 4 on 5\n",
      "4 on 24 epoch 4 on 5\n",
      "5 on 24 epoch 4 on 5\n",
      "6 on 24 epoch 4 on 5\n",
      "7 on 24 epoch 4 on 5\n",
      "8 on 24 epoch 4 on 5\n",
      "9 on 24 epoch 4 on 5\n",
      "10 on 24 epoch 4 on 5\n",
      "11 on 24 epoch 4 on 5\n",
      "12 on 24 epoch 4 on 5\n",
      "13 on 24 epoch 4 on 5\n",
      "14 on 24 epoch 4 on 5\n",
      "15 on 24 epoch 4 on 5\n",
      "16 on 24 epoch 4 on 5\n",
      "17 on 24 epoch 4 on 5\n",
      "18 on 24 epoch 4 on 5\n",
      "19 on 24 epoch 4 on 5\n",
      "20 on 24 epoch 4 on 5\n",
      "21 on 24 epoch 4 on 5\n",
      "22 on 24 epoch 4 on 5\n",
      "23 on 24 epoch 4 on 5\n",
      "0.0027740977335876473\n",
      "fin train\n"
     ]
    }
   ],
   "source": [
    "# Launch training\n",
    "print('début train')\n",
    "losses = []\n",
    "ids_iter=list(range(len(inputs_embeddings)))\n",
    "for epoch in range(nb_epoch):\n",
    "    print('epoch',epoch,'on',nb_epoch)\n",
    "    shuffle(ids_iter)\n",
    "    losses_ = []\n",
    "    #scheduler.step(epoch)\n",
    "    #for id_, it_ in enumerate(iter_):\n",
    "    for id_, it_ in enumerate(ids_iter):\n",
    "        print(id_,'on',len(ids_iter),'epoch',epoch,'on',nb_epoch)\n",
    "        sentences_emb = inputs_embeddings[it_] #(8,32,4096)\n",
    "        ref = outputs_refs[it_] #(1,32)\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        #print(i, nb_sentences, taille_context)\n",
    "        #model.zero_grad()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        if type_sentence_embedding == 'lstm':\n",
    "            model.hidden_sentences = model.init_hidden(batch_size=sentences_emb.shape[1])\n",
    "        model.hidden = model.init_hidden(batch_size=sentences_emb.shape[1])\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        prediction = model(sentences_emb) #(32,1)    #(1,32,4096) or (109,8*32,300) ?\n",
    "\n",
    "        prediction = torch.squeeze(prediction, 1)\n",
    "        ref = torch.squeeze(ref, 0)\n",
    "        #print(prediction.shape, ref.shape)\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = criterion(prediction, ref) #targets)\n",
    "        losses_.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #break\n",
    "    print(sum(losses_)/len(losses_))\n",
    "    losses.append(losses_)\n",
    "    #model.get_prediction(X_, Y_, idx_set_words, embed, model, taille_context=taille_context, device=device)\n",
    "    #break\n",
    "    torch.save(model.state_dict(), '/people/maurice/HierarchicalRNN/last_model.pth.tar')\n",
    "    #break\n",
    "print('fin train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 300]) torch.Size([32, 300, 4]) torch.Size([32, 4, 1])\n",
      "torch.Size([32, 300, 1])\n",
      "torch.Size([1, 32, 300])\n",
      "torch.Size([1, 32, 300])\n",
      "SUM torch.Size([1, 32, 1200])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4,32,300)\n",
    "b = torch.ones(32,4,1)\n",
    "aa = a.transpose(0,1).transpose(1,2)\n",
    "print(a.shape, aa.shape, b.shape)\n",
    "m = torch.matmul(aa, b)\n",
    "print(m.shape) #torch.Size([32, 300, 1])\n",
    "mm = m.transpose(0,1).transpose(0,2)\n",
    "print(mm.shape) #torch.Size([1, 32, 300])\n",
    "m.transpose(0,1).transpose(0,2).shape #(300,32,1) -> (32,300,1) -> torch.Size([1, 300, 32])\n",
    "#(1,32,300)\n",
    "print(torch.unsqueeze(a[-1,:,:],0).shape)\n",
    "sum_ = torch.cat((torch.unsqueeze(a[-1,:,:],0), torch.unsqueeze(a[-1,:,:],0), mm, mm), -1)\n",
    "print('SUM', sum_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 0 on 176\n",
      "Found 927(/934) words with w2v vectors\n",
      "Vocab size : 927\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-58c858684d57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0midx_set_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_trained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaille_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaille_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargset_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_trained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_trained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_sentence_embedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype_sentence_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_trained\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel_trained\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/people/maurice/HierarchicalRNN/last_model.pth.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HierarchicalRNN/model.py\u001b[0m in \u001b[0;36mlaunch_train\u001b[0;34m(X_all, Y_all, words_set, we, taille_embedding, batch_size, hidden_size, taille_context, bidirectional, num_layers, nb_epoch, targset_size, device, is_trained, type_sentence_embedding)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoucentage_majority_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_Y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(8..)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;31m#TODO CURRENTLY ONLY FOR INFERSENT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0minputs_embeddings_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_refs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_by_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaille_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(n/32,8,32,4096) and (n/32,1,32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0minputs_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeddings_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0moutputs_refs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_refs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HierarchicalRNN/model.py\u001b[0m in \u001b[0;36msplit_by_context\u001b[0;34m(X, Y, taille_context, batch_size)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mlist_tensors_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtaille_context\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#NOT 1 NOW 0 FOR THE NUMBER OF SENTENCES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mlist_tensors_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaille_context\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(8,1,4096)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0mlist_tensors_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtaille_context\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mtensor_split_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_tensors_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaille_context\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(n-8,8,1,4096) -> (8,n,1,4096) -> (8,n,4096)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "import model\n",
    "idx_set_words, embed, model_trained, losses = model.launch_train(X_train, Y_train, words_set, we, taille_embedding, batch_size=32, hidden_size=hidden_size, taille_context=3, bidirectional=False, num_layers=1, nb_epoch=100, targset_size=1, device=device, is_trained=is_trained, type_sentence_embedding=type_sentence_embedding)\n",
    "if is_trained:\n",
    "    model_trained.load_state_dict(torch.load('/people/maurice/HierarchicalRNN/last_model.pth.tar'))\n",
    "else:\n",
    "    np.save('losses.npy', np.asarray(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_predictions(X_train, Y_train, idx_set_words, embed, model_trained, batch_size=32, taille_context=3, device=device, is_eval=True, type_sentence_embedding=type_sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1,2,3], [4,5,6]]\n",
    "import pickle\n",
    "import numpy as np\n",
    "b = np.asarray(a)\n",
    "np.save('b.npy', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10154 10153\n"
     ]
    }
   ],
   "source": [
    "a = range(10161 - (2*3 + 1))\n",
    "print(len(a), a[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(X, Y, idx_set_words, embed, model, taille_embedding, taille_context=3, device='cpu', is_eval=False):\n",
    "    if is_eval:\n",
    "        model.eval()\n",
    "    vectorized_seqs = [[idx_set_words[w] for w in s]for s in X]\n",
    "    words_embeddings = create_X(X, vectorized_seqs, device)\n",
    "    sentences_embeddings = sentence_embeddings_by_sum(words_embeddings, embed, vectorized_seqs, taille_embedding)\n",
    "    Y = create_Y(Y, device)\n",
    "    nb_sentences = len(vectorized_seqs)\n",
    "    #print('nb_sentences', nb_sentences)\n",
    "    \n",
    "    # See what the scores are after training\n",
    "    with torch.no_grad():\n",
    "        error_global = 0\n",
    "        iter_sentences = range(nb_sentences - (2*taille_context + 1))\n",
    "        for i in iter_sentences:\n",
    "            indices_previous = torch.tensor(list(range(i,i+taille_context+1)), device=device)\n",
    "            indices_future = torch.tensor(list(range(i+2*taille_context+1,i+taille_context,-1)), device=device)\n",
    "            input_previous_features = torch.index_select(sentences_embeddings, 0, indices_previous)\n",
    "            input_future_features = torch.index_select(sentences_embeddings, 0, indices_future)\n",
    "            prediction = model(input_previous_features, input_future_features).item()\n",
    "            ref = Y[i+taille_context].item()\n",
    "            #print(prediction, ref)\n",
    "            if abs(ref - prediction) >= 0.5:\n",
    "                error_global += 1\n",
    "        error_global /= len(iter_sentences)\n",
    "        print('error_global', error_global)\n",
    "        \n",
    "X_train, Y_train, X_dev, Y_dev, X_test, Y_test, words_set, we = data.load_data()\n",
    "taille_embedding = len(we.vectors[we.stoi[X_train[0][0]]])\n",
    "\n",
    "idx_set_words, embed, model_trained, losses = model.launch_train(X_train, Y_train, words_set, we, taille_embedding, taille_context=3, bidirectional=False, num_layers=3, nb_epoch=100, targset_size=1, device=device, is_trained=is_trained)\n",
    "model_trained.load_state_dict(torch.load('/people/maurice/HierarchicalRNN/last_model.pth.tar'))\n",
    "\n",
    "model.get_prediction(X_train, Y_train, idx_set_words, embed, model_trained, taille_embedding, taille_context=3, device=device, is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "losses = np.load('HierarchicalRNN/losses.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6854, device='cuda:0', requires_grad=True)\n",
      "tensor(0.4376, device='cuda:0', requires_grad=True)\n",
      "tensor(0.4048, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3985, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3987, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3986, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3985, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3984, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3983, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3985, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3984, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3983, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3983, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3983, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3983, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3983, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3983, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3983, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3983, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3983, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3983, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3983, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3983, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3983, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3983, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3983, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3983, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3983, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3984, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3984, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3984, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3984, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3985, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3985, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3985, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3985, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3986, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3986, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3986, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3986, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3986, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3986, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3986, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3985, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3985, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3986, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3986, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3986, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3986, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3986, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3986, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3986, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3986, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3987, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3987, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3987, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3987, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3987, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3987, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3988, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3988, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3988, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.3990, device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in range(losses.shape[0]):\n",
    "    print(losses[i,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12701 8721 3980 0.6866388473348555 0.3133611526651445\n"
     ]
    }
   ],
   "source": [
    "def count_change(Y):\n",
    "    Y_positive = [y for y in Y if y == 1]\n",
    "    return len(Y_positive)\n",
    "\n",
    "print(len(Y), count_change(Y), len(Y) - count_change(Y), count_change(Y)/len(Y), 1-count_change(Y)/len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_positives = np.load('HierarchicalRNN/Y_positives.npy')\n",
    "Y_negatives = np.load('HierarchicalRNN/Y_negatives.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Y_positives)\n",
    "plt.plot(Y_negatives)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(Y_positives, bins='auto')\n",
    "plt.hist(Y_negatives, bins='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyannote'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-cb2be66b6c4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyannote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbinary_classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyannote'"
     ]
    }
   ],
   "source": [
    "from pyannote.metrics import binary_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load corpus dataset\n",
      "to del t1\n",
      "to del 212\n",
      "to del koothrapali\n",
      "to del 1974\n",
      "to del 360\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "import numpy as np\n",
    "import itertools\n",
    "import csv\n",
    "from joblib import Parallel, delayed\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from random import shuffle\n",
    "from random import sample\n",
    "from models import InferSent\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "\n",
    "import torchtext.vocab as vocab\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def load_data(config, path_transcripts='/vol/work2/galmant/transcripts/'):\n",
    "    type_sentence_embedding = config['type_sentence_embedding']\n",
    "    dev_set_list = config['dev_set_list']\n",
    "    test_set_list = config['test_set_list']\n",
    "    \n",
    "    punctuations_end_sentence = ['.', '?', '!']\n",
    "    punctuations = string.punctuation #['!','(',')',',','-','.','/',':',';','<','=','>','?','[','\\\\',']','^','_','{','|','}','~'] #!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "\n",
    "    we = None\n",
    "    if type_sentence_embedding == 'lstm':\n",
    "        we = vocab.FastText(language='en')\n",
    "\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_dev = []\n",
    "    Y_dev = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    words_set = set()\n",
    "    for file in sorted(glob.glob(path_transcripts+'*')):\n",
    "        with open(file, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "            X_ = []\n",
    "            Y_ = []\n",
    "            for row in reader:\n",
    "                #print(row)\n",
    "                sentence = row[2]\n",
    "                old_word = row[2]\n",
    "                for word in row[3:]:\n",
    "                    if any(punctuation in old_word for punctuation in punctuations_end_sentence) and word and word[0].isupper():\n",
    "                        sentence = sentence.strip()\n",
    "                        n = 0\n",
    "                        for i,s in enumerate(sentence):\n",
    "                            if s in punctuations:\n",
    "                                sentence_ = list(sentence)\n",
    "                                sentence_.insert(i + n + 1,' ')\n",
    "                                sentence_.insert(i + n,' ')\n",
    "                                sentence = ''.join(sentence_)\n",
    "                                n += 2\n",
    "                        #print(sentence)\n",
    "                        X_.append(sentence)\n",
    "                        Y_.append(row[1])\n",
    "                        sentence = word\n",
    "                    else:\n",
    "                        sentence += ' '+word\n",
    "                    old_word = word\n",
    "                if sentence and row[1]:\n",
    "                    sentence = sentence.strip()\n",
    "                    n = 0\n",
    "                    for i,s in enumerate(sentence):\n",
    "                        if s in punctuations:\n",
    "                            sentence_ = list(sentence)\n",
    "                            sentence_.insert(i + n + 1,' ')\n",
    "                            sentence_.insert(i + n,' ')\n",
    "                            sentence = ''.join(sentence_)\n",
    "                            n += 2\n",
    "                    #print(sentence)\n",
    "                    X_.append(sentence)\n",
    "                    Y_.append(row[1])\n",
    "            Y = [s.lower() for s in Y_]\n",
    "            if type_sentence_embedding == 'lstm':\n",
    "                X = [s.lower().split() for s in X_]\n",
    "                #print(X)\n",
    "                #Y = [s.lower() for s in Y_]\n",
    "                to_del = []\n",
    "                for s in X:\n",
    "                    #print(s)\n",
    "                    for w in s:\n",
    "                        #print(w)\n",
    "                        if w not in we.stoi:\n",
    "                            to_del.append(w)\n",
    "                            print('to del', w)\n",
    "                X = [[w.strip() for w in s if w not in to_del] for s in X]\n",
    "                for words_per_sentence in X:\n",
    "                    words_set = words_set.union(set(words_per_sentence))\n",
    "            else:\n",
    "                X = X_\n",
    "                Y = Y#_\n",
    "            if len(X)>0 and len(Y)>0:\n",
    "                names_episode = file.split('/')[-1]\n",
    "                names_season = '.'.join(names_episode.split('.')[:-1])\n",
    "                names_serie = '.'.join(names_episode.split('.')[0])\n",
    "                if names_episode in dev_set_list or names_season in dev_set_list or names_serie in dev_set_list:\n",
    "                    X_dev.append(X)\n",
    "                    Y_dev.append(Y)\n",
    "                elif names_episode in test_set_list or names_season in test_set_list or names_serie in test_set_list:\n",
    "                    X_test.append(X)\n",
    "                    Y_test.append(Y)\n",
    "                else:\n",
    "                    X_train.append(X)\n",
    "                    Y_train.append(Y)\n",
    "            assert len(X) == len(Y)\n",
    "        #print(X)\n",
    "        break\n",
    "    return X_train, Y_train, X_dev, Y_dev, X_test, Y_test, words_set, we\n",
    "\n",
    "config = {}\n",
    "config['dev_set_list']=['TheBigBangTheory.Season02']\n",
    "config['test_set_list']=['TheBigBangTheory.Season01']\n",
    "config['type_sentence_embedding']='lstm'#'infersent'\n",
    "print('Load corpus dataset')\n",
    "X_train, Y_train, X_dev, Y_dev, X_test, Y_test, words_set, we = load_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-17_05:40:44\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "x = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\").replace(' ','_')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBT.S1\n",
      "*.S1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a = ['BBT.S1', '*.S1']\n",
    "for a_ in a:\n",
    "    print(a_)\n",
    "    if '*' in a_:\n",
    "        print('True')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.6Conda5.1GPU",
   "language": "python",
   "name": "py3.6gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
